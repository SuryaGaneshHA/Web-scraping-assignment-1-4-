{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c924ec0-f6ac-4bf8-a9ad-f35865a794b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "934b2c79-6647-4fdf-bcd7-d14a7826ed0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank: \"Baby Shark Dance\"[7]\n",
      "Name: Pinkfong Baby Shark - Kids' Songs & Stories\n",
      "Artist: 14.32\n",
      "Upload Date: June 17, 2016\n",
      "Views: [A]\n",
      "\n",
      "Rank: \"Despacito\"[10]\n",
      "Name: Luis Fonsi\n",
      "Artist: 8.41\n",
      "Upload Date: January 12, 2017\n",
      "Views: [B]\n",
      "\n",
      "Rank: \"Johny Johny Yes Papa\"[18]\n",
      "Name: LooLoo Kids - Nursery Rhymes and Children's Songs\n",
      "Artist: 6.89\n",
      "Upload Date: October 8, 2016\n",
      "Views: \n",
      "\n",
      "Rank: \"Bath Song\"[19]\n",
      "Name: Cocomelon - Nursery Rhymes\n",
      "Artist: 6.66\n",
      "Upload Date: May 2, 2018\n",
      "Views: \n",
      "\n",
      "Rank: \"Shape of You\"[20]\n",
      "Name: Ed Sheeran\n",
      "Artist: 6.23\n",
      "Upload Date: January 30, 2017\n",
      "Views: [C]\n",
      "\n",
      "Rank: \"See You Again\"[23]\n",
      "Name: Wiz Khalifa\n",
      "Artist: 6.22\n",
      "Upload Date: April 6, 2015\n",
      "Views: [D]\n",
      "\n",
      "Rank: \"Wheels on the Bus\"[28]\n",
      "Name: Cocomelon - Nursery Rhymes\n",
      "Artist: 6.01\n",
      "Upload Date: May 24, 2018\n",
      "Views: \n",
      "\n",
      "Rank: \"Phonics Song with Two Words\"[29]\n",
      "Name: ChuChu TV Nursery Rhymes & Kids Songs\n",
      "Artist: 5.75\n",
      "Upload Date: March 6, 2014\n",
      "Views: \n",
      "\n",
      "Rank: \"Uptown Funk\"[30]\n",
      "Name: Mark Ronson\n",
      "Artist: 5.18\n",
      "Upload Date: November 19, 2014\n",
      "Views: \n",
      "\n",
      "Rank: \"Gangnam Style\"[31]\n",
      "Name: Psy\n",
      "Artist: 5.10\n",
      "Upload Date: July 15, 2012\n",
      "Views: [E]\n",
      "\n",
      "Rank: \"Learning Colors – Colorful Eggs on a Farm\"[36]\n",
      "Name: Miroshka TV\n",
      "Artist: 5.09\n",
      "Upload Date: February 27, 2018\n",
      "Views: \n",
      "\n",
      "Rank: \"Dame Tu Cosita\"[37]\n",
      "Name: Ultra Records\n",
      "Artist: 4.59\n",
      "Upload Date: April 5, 2018\n",
      "Views: \n",
      "\n",
      "Rank: \"Masha and the Bear – Recipe for Disaster\"[38]\n",
      "Name: Get Movies\n",
      "Artist: 4.57\n",
      "Upload Date: January 31, 2012\n",
      "Views: \n",
      "\n",
      "Rank: \"Axel F\"[39]\n",
      "Name: Crazy Frog\n",
      "Artist: 4.45\n",
      "Upload Date: June 16, 2009\n",
      "Views: \n",
      "\n",
      "Rank: \"Sugar\"[40]\n",
      "Name: Maroon 5\n",
      "Artist: 4.02\n",
      "Upload Date: January 14, 2015\n",
      "Views: \n",
      "\n",
      "Rank: \"Baa Baa Black Sheep\"[41]\n",
      "Name: Cocomelon - Nursery Rhymes\n",
      "Artist: 4.01\n",
      "Upload Date: June 25, 2018\n",
      "Views: \n",
      "\n",
      "Rank: \"Counting Stars\"[42]\n",
      "Name: OneRepublic\n",
      "Artist: 4.00\n",
      "Upload Date: May 31, 2013\n",
      "Views: \n",
      "\n",
      "Rank: \"Lakdi Ki Kathi\"[43]\n",
      "Name: Jingle Toons\n",
      "Artist: 3.98\n",
      "Upload Date: June 14, 2018\n",
      "Views: \n",
      "\n",
      "Rank: \"Roar\"[44]\n",
      "Name: Katy Perry\n",
      "Artist: 3.98\n",
      "Upload Date: September 5, 2013\n",
      "Views: \n",
      "\n",
      "Rank: \"Waka Waka (This Time for Africa)\"[45]\n",
      "Name: Shakira\n",
      "Artist: 3.89\n",
      "Upload Date: June 4, 2010\n",
      "Views: \n",
      "\n",
      "Rank: \"Sorry\"[46]\n",
      "Name: Justin Bieber\n",
      "Artist: 3.78\n",
      "Upload Date: October 22, 2015\n",
      "Views: \n",
      "\n",
      "Rank: \"Shree Hanuman Chalisa\"[47]\n",
      "Name: T-Series Bhakti Sagar\n",
      "Artist: 3.77\n",
      "Upload Date: May 10, 2011\n",
      "Views: \n",
      "\n",
      "Rank: \"Humpty the train on a fruits ride\"[48]\n",
      "Name: Kiddiestv Hindi - Nursery Rhymes & Kids Songs\n",
      "Artist: 3.76\n",
      "Upload Date: January 26, 2018\n",
      "Views: \n",
      "\n",
      "Rank: \"Thinking Out Loud\"[49]\n",
      "Name: Ed Sheeran\n",
      "Artist: 3.75\n",
      "Upload Date: October 7, 2014\n",
      "Views: \n",
      "\n",
      "Rank: \"Perfect\"[50]\n",
      "Name: Ed Sheeran\n",
      "Artist: 3.70\n",
      "Upload Date: November 9, 2017\n",
      "Views: \n",
      "\n",
      "Rank: \"Dark Horse\"[51]\n",
      "Name: Katy Perry\n",
      "Artist: 3.70\n",
      "Upload Date: February 20, 2014\n",
      "Views: \n",
      "\n",
      "Rank: \"Let Her Go\"[52]\n",
      "Name: Passenger\n",
      "Artist: 3.64\n",
      "Upload Date: July 25, 2012\n",
      "Views: \n",
      "\n",
      "Rank: \"Faded\"[53]\n",
      "Name: Alan Walker\n",
      "Artist: 3.60\n",
      "Upload Date: December 3, 2015\n",
      "Views: \n",
      "\n",
      "Rank: \"Girls Like You\"[54]\n",
      "Name: Maroon 5\n",
      "Artist: 3.58\n",
      "Upload Date: May 31, 2018\n",
      "Views: \n",
      "\n",
      "Rank: \"Lean On\"[55]\n",
      "Name: Major Lazer Official\n",
      "Artist: 3.57\n",
      "Upload Date: March 22, 2015\n",
      "Views: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Wikipedia page\n",
    "url = \"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the most-viewed YouTube videos\n",
    "table = soup.find('table', class_='wikitable')\n",
    "\n",
    "# Initialize lists to store the details\n",
    "rank_list = []\n",
    "name_list = []\n",
    "artist_list = []\n",
    "upload_date_list = []\n",
    "views_list = []\n",
    "\n",
    "# Iterate through each row in the table\n",
    "for row in table.find_all('tr')[1:]:\n",
    "    # Extract data from each column in the row\n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) >= 5:  # Check if the row has at least 5 columns\n",
    "        rank = columns[0].text.strip()\n",
    "        name = columns[1].text.strip()\n",
    "        artist = columns[2].text.strip()\n",
    "        upload_date = columns[3].text.strip()\n",
    "        views = columns[4].text.strip()\n",
    "        \n",
    "        # Append data to respective lists\n",
    "        rank_list.append(rank)\n",
    "        name_list.append(name)\n",
    "        artist_list.append(artist)\n",
    "        upload_date_list.append(upload_date)\n",
    "        views_list.append(views)\n",
    "\n",
    "# Print the details of the most-viewed videos\n",
    "for i in range(len(rank_list)):\n",
    "    print(\"Rank:\", rank_list[i])\n",
    "    print(\"Name:\", name_list[i])\n",
    "    print(\"Artist:\", artist_list[i])\n",
    "    print(\"Upload Date:\", upload_date_list[i])\n",
    "    print(\"Views:\", views_list[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06768268-82cb-4045-84ad-1114fd61e015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abdde6d-af7f-438b-a4da-29043642c5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "International fixtures link not found on the homepage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the BCCI homepage\n",
    "homepage_url = \"https://www.bcci.tv/\"\n",
    "\n",
    "# Send a GET request to the homepage URL\n",
    "homepage_response = requests.get(homepage_url)\n",
    "\n",
    "# Parse the homepage HTML content\n",
    "homepage_soup = BeautifulSoup(homepage_response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the international fixtures page\n",
    "fixtures_link = homepage_soup.find('a', class_='international-drop-down')\n",
    "\n",
    "if fixtures_link:\n",
    "    fixtures_url = homepage_url + fixtures_link['href']\n",
    "    \n",
    "    # Send a GET request to the international fixtures page\n",
    "    fixtures_response = requests.get(fixtures_url)\n",
    "    \n",
    "    # Parse the fixtures page HTML content\n",
    "    fixtures_soup = BeautifulSoup(fixtures_response.text, 'html.parser')\n",
    "    \n",
    "    # Find the section containing the fixtures\n",
    "    fixtures_section = fixtures_soup.find('section', class_='js-list')\n",
    "\n",
    "    # Initialize lists to store the details\n",
    "    series_list = []\n",
    "    place_list = []\n",
    "    date_list = []\n",
    "    time_list = []\n",
    "\n",
    "    # Extract data from each fixture\n",
    "    fixtures = fixtures_section.find_all('div', class_='fixture__info')\n",
    "    for fixture in fixtures:\n",
    "        series = fixture.find('p', class_='fixture__additional-info').text.strip()\n",
    "        place = fixture.find('span', class_='u-unskewed-text').text.strip()\n",
    "        date = fixture.find('span', class_='fixture__date').text.strip()\n",
    "        time = fixture.find('span', class_='fixture__time').text.strip()\n",
    "        \n",
    "        # Append data to respective lists\n",
    "        series_list.append(series)\n",
    "        place_list.append(place)\n",
    "        date_list.append(date)\n",
    "        time_list.append(time)\n",
    "\n",
    "    # Print the details of the fixtures\n",
    "    for i in range(len(series_list)):\n",
    "        print(\"Series:\", series_list[i])\n",
    "        print(\"Place:\", place_list[i])\n",
    "        print(\"Date:\", date_list[i])\n",
    "        print(\"Time:\", time_list[i])\n",
    "        print()\n",
    "else:\n",
    "    print(\"International fixtures link not found on the homepage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f9ab76-532e-44e5-81d6-5a8c2bca25fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cd22b42-d111-407a-ada4-8fcaa351d938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to the economy page not found on the homepage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the statisticstimes homepage\n",
    "homepage_url = \"http://statisticstimes.com/\"\n",
    "\n",
    "# Send a GET request to the homepage URL\n",
    "homepage_response = requests.get(homepage_url)\n",
    "\n",
    "# Parse the homepage HTML content\n",
    "homepage_soup = BeautifulSoup(homepage_response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the economy page\n",
    "economy_link = homepage_soup.find('a', href='/economy')\n",
    "\n",
    "if economy_link:\n",
    "    economy_url = homepage_url + economy_link['href']\n",
    "    \n",
    "    # Send a GET request to the economy page\n",
    "    economy_response = requests.get(economy_url)\n",
    "    \n",
    "    # Parse the economy page HTML content\n",
    "    economy_soup = BeautifulSoup(economy_response.text, 'html.parser')\n",
    "    \n",
    "    # Find the link to the Indian states GDP page\n",
    "    india_gdp_link = economy_soup.find('a', href='/india/statistics/gdp-at-current-prices.php')\n",
    "\n",
    "    if india_gdp_link:\n",
    "        india_gdp_url = homepage_url + india_gdp_link['href']\n",
    "        \n",
    "        # Send a GET request to the Indian states GDP page\n",
    "        india_gdp_response = requests.get(india_gdp_url)\n",
    "        \n",
    "        # Parse the Indian states GDP page HTML content\n",
    "        india_gdp_soup = BeautifulSoup(india_gdp_response.text, 'html.parser')\n",
    "        \n",
    "        # Find the table containing the state-wise GDP details\n",
    "        table = india_gdp_soup.find('table', class_='display')\n",
    "\n",
    "        # Initialize lists to store the details\n",
    "        rank_list = []\n",
    "        state_list = []\n",
    "        gdp_18_19_list = []\n",
    "        gdp_19_20_list = []\n",
    "        share_18_19_list = []\n",
    "        gdp_billion_list = []\n",
    "\n",
    "        # Extract data from each row in the table\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows[1:]:\n",
    "            columns = row.find_all('td')\n",
    "            rank = columns[0].text.strip()\n",
    "            state = columns[1].text.strip()\n",
    "            gdp_18_19 = columns[2].text.strip()\n",
    "            gdp_19_20 = columns[3].text.strip()\n",
    "            share_18_19 = columns[4].text.strip()\n",
    "            gdp_billion = columns[5].text.strip()\n",
    "\n",
    "            # Append data to respective lists\n",
    "            rank_list.append(rank)\n",
    "            state_list.append(state)\n",
    "            gdp_18_19_list.append(gdp_18_19)\n",
    "            gdp_19_20_list.append(gdp_19_20)\n",
    "            share_18_19_list.append(share_18_19)\n",
    "            gdp_billion_list.append(gdp_billion)\n",
    "\n",
    "        # Print the details of state-wise GDP\n",
    "        for i in range(len(rank_list)):\n",
    "            print(\"Rank:\", rank_list[i])\n",
    "            print(\"State:\", state_list[i])\n",
    "            print(\"GSDP(18-19) - at current prices:\", gdp_18_19_list[i])\n",
    "            print(\"GSDP(19-20) - at current prices:\", gdp_19_20_list[i])\n",
    "            print(\"Share(18-19):\", share_18_19_list[i])\n",
    "            print(\"GDP($ billion):\", gdp_billion_list[i])\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Link to Indian states GDP page not found on the economy page.\")\n",
    "else:\n",
    "    print(\"Link to the economy page not found on the homepage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e3bf18-befb-427d-b223-1b148a936d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38ccb7df-9718-4a48-9283-01ed8985dd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore menu not found on the homepage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Github homepage\n",
    "homepage_url = \"https://github.com/\"\n",
    "\n",
    "# Send a GET request to the homepage URL\n",
    "homepage_response = requests.get(homepage_url)\n",
    "\n",
    "# Parse the homepage HTML content\n",
    "homepage_soup = BeautifulSoup(homepage_response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the trending page under Explore menu\n",
    "explore_menu = homepage_soup.find('nav', class_='mr-4')\n",
    "if explore_menu:\n",
    "    trending_link = explore_menu.find('a', href='/trending')\n",
    "\n",
    "    if trending_link:\n",
    "        trending_url = \"https://github.com/trending\"\n",
    "        \n",
    "        # Send a GET request to the trending page\n",
    "        trending_response = requests.get(trending_url)\n",
    "        \n",
    "        # Parse the trending page HTML content\n",
    "        trending_soup = BeautifulSoup(trending_response.text, 'html.parser')\n",
    "        \n",
    "        # Find the list of trending repositories\n",
    "        repo_list = trending_soup.find_all('article', class_='Box-row')\n",
    "\n",
    "        # Initialize lists to store the details\n",
    "        repo_title_list = []\n",
    "        repo_description_list = []\n",
    "        contributors_count_list = []\n",
    "        language_used_list = []\n",
    "\n",
    "        # Extract data from each trending repository\n",
    "        for repo in repo_list:\n",
    "            repo_title_elem = repo.find('h1', class_='h3 lh-condensed')\n",
    "            repo_description_elem = repo.find('p', class_='col-9 color-text-secondary my-1 pr-4')\n",
    "            contributors_count_elem = repo.find('a', href=True, class_='Link--muted d-inline-block mr-3')\n",
    "            language_used_elem = repo.find('span', class_='d-inline-block ml-0 mr-3')\n",
    "\n",
    "            # Check if elements exist before extracting data\n",
    "            if repo_title_elem:\n",
    "                repo_title = repo_title_elem.text.strip()\n",
    "            else:\n",
    "                repo_title = \"N/A\"\n",
    "\n",
    "            if repo_description_elem:\n",
    "                repo_description = repo_description_elem.text.strip()\n",
    "            else:\n",
    "                repo_description = \"N/A\"\n",
    "\n",
    "            if contributors_count_elem:\n",
    "                contributors_count = contributors_count_elem.text.strip()\n",
    "            else:\n",
    "                contributors_count = \"N/A\"\n",
    "\n",
    "            if language_used_elem:\n",
    "                language_used = language_used_elem.text.strip()\n",
    "            else:\n",
    "                language_used = \"N/A\"\n",
    "            \n",
    "            # Append data to respective lists\n",
    "            repo_title_list.append(repo_title)\n",
    "            repo_description_list.append(repo_description)\n",
    "            contributors_count_list.append(contributors_count)\n",
    "            language_used_list.append(language_used)\n",
    "\n",
    "        # Print the details of trending repositories\n",
    "        for i in range(len(repo_title_list)):\n",
    "            print(\"Repository Title:\", repo_title_list[i])\n",
    "            print(\"Repository Description:\", repo_description_list[i])\n",
    "            print(\"Contributors Count:\", contributors_count_list[i])\n",
    "            print(\"Language Used:\", language_used_list[i])\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Trending link not found under Explore menu on the homepage.\")\n",
    "else:\n",
    "    print(\"Explore menu not found on the homepage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68932e34-c9af-42c5-9407-24ba7ff5813a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a41ee0-f46e-4313-8c14-9910fc9c3107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charts menu not found on the homepage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the Billboard homepage\n",
    "homepage_url = \"https://www.billboard.com/\"\n",
    "\n",
    "# Send a GET request to the homepage URL\n",
    "homepage_response = requests.get(homepage_url)\n",
    "\n",
    "# Parse the homepage HTML content\n",
    "homepage_soup = BeautifulSoup(homepage_response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the Hot 100 page under the Charts menu\n",
    "charts_menu = homepage_soup.find('div', class_='header__main-menu__wrapper')\n",
    "if charts_menu:\n",
    "    hot_100_link = charts_menu.find('a', href='/charts/hot-100')\n",
    "\n",
    "    if hot_100_link:\n",
    "        hot_100_url = \"https://www.billboard.com/charts/hot-100\"\n",
    "        \n",
    "        # Send a GET request to the Hot 100 page\n",
    "        hot_100_response = requests.get(hot_100_url)\n",
    "        \n",
    "        # Parse the Hot 100 page HTML content\n",
    "        hot_100_soup = BeautifulSoup(hot_100_response.text, 'html.parser')\n",
    "        \n",
    "        # Find the list of songs\n",
    "        songs_list = hot_100_soup.find_all('li', class_='chart-list__element')\n",
    "\n",
    "        # Initialize lists to store the details\n",
    "        song_name_list = []\n",
    "        artist_name_list = []\n",
    "        last_week_rank_list = []\n",
    "        peak_rank_list = []\n",
    "        weeks_on_board_list = []\n",
    "\n",
    "        # Extract data from each song\n",
    "        for song in songs_list:\n",
    "            song_name_elem = song.find('span', class_='chart-element__information__song')\n",
    "            artist_name_elem = song.find('span', class_='chart-element__information__artist')\n",
    "            last_week_rank_elem = song.find('span', class_='chart-element__meta text--center color--secondary text--last')\n",
    "            peak_rank_elem = song.find('span', class_='chart-element__meta text--center color--secondary text--peak')\n",
    "            weeks_on_board_elem = song.find('span', class_='chart-element__meta text--center color--secondary text--week')\n",
    "\n",
    "            # Check if elements exist before extracting data\n",
    "            if song_name_elem:\n",
    "                song_name = song_name_elem.text.strip()\n",
    "            else:\n",
    "                song_name = \"N/A\"\n",
    "\n",
    "            if artist_name_elem:\n",
    "                artist_name = artist_name_elem.text.strip()\n",
    "            else:\n",
    "                artist_name = \"N/A\"\n",
    "\n",
    "            if last_week_rank_elem:\n",
    "                last_week_rank = last_week_rank_elem.text.strip()\n",
    "            else:\n",
    "                last_week_rank = \"N/A\"\n",
    "\n",
    "            if peak_rank_elem:\n",
    "                peak_rank = peak_rank_elem.text.strip()\n",
    "            else:\n",
    "                peak_rank = \"N/A\"\n",
    "\n",
    "            if weeks_on_board_elem:\n",
    "                weeks_on_board = weeks_on_board_elem.text.strip()\n",
    "            else:\n",
    "                weeks_on_board = \"N/A\"\n",
    "            \n",
    "            # Append data to respective lists\n",
    "            song_name_list.append(song_name)\n",
    "            artist_name_list.append(artist_name)\n",
    "            last_week_rank_list.append(last_week_rank)\n",
    "            peak_rank_list.append(peak_rank)\n",
    "            weeks_on_board_list.append(weeks_on_board)\n",
    "\n",
    "        # Print the details of the top 100 songs\n",
    "        for i in range(len(song_name_list)):\n",
    "            print(\"Song Name:\", song_name_list[i])\n",
    "            print(\"Artist Name:\", artist_name_list[i])\n",
    "            print(\"Last Week Rank:\", last_week_rank_list[i])\n",
    "            print(\"Peak Rank:\", peak_rank_list[i])\n",
    "            print(\"Weeks on Board:\", weeks_on_board_list[i])\n",
    "            print()\n",
    "    else:\n",
    "        print(\"Hot 100 link not found under Charts menu on the homepage.\")\n",
    "else:\n",
    "    print(\"Charts menu not found on the homepage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c52a9fc8-c03a-4e9b-ae8e-8b3dca0dfee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3515b3a3-0ea6-4ab7-8253-8bebc70dd10c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Book Name: 1\n",
      "Author Name: Da Vinci Code,The\n",
      "Volumes Sold: Brown, Dan\n",
      "Publisher: 5,094,805\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 2\n",
      "Author Name: Harry Potter and the Deathly Hallows\n",
      "Volumes Sold: Rowling, J.K.\n",
      "Publisher: 4,475,152\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 3\n",
      "Author Name: Harry Potter and the Philosopher's Stone\n",
      "Volumes Sold: Rowling, J.K.\n",
      "Publisher: 4,200,654\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 4\n",
      "Author Name: Harry Potter and the Order of the Phoenix\n",
      "Volumes Sold: Rowling, J.K.\n",
      "Publisher: 4,179,479\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 5\n",
      "Author Name: Fifty Shades of Grey\n",
      "Volumes Sold: James, E. L.\n",
      "Publisher: 3,758,936\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 6\n",
      "Author Name: Harry Potter and the Goblet of Fire\n",
      "Volumes Sold: Rowling, J.K.\n",
      "Publisher: 3,583,215\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 7\n",
      "Author Name: Harry Potter and the Chamber of Secrets\n",
      "Volumes Sold: Rowling, J.K.\n",
      "Publisher: 3,484,047\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 8\n",
      "Author Name: Harry Potter and the Prisoner of Azkaban\n",
      "Volumes Sold: Rowling, J.K.\n",
      "Publisher: 3,377,906\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 9\n",
      "Author Name: Angels and Demons\n",
      "Volumes Sold: Brown, Dan\n",
      "Publisher: 3,193,946\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 10\n",
      "Author Name: Harry Potter and the Half-blood Prince:Children's Edition\n",
      "Volumes Sold: Rowling, J.K.\n",
      "Publisher: 2,950,264\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 11\n",
      "Author Name: Fifty Shades Darker\n",
      "Volumes Sold: James, E. L.\n",
      "Publisher: 2,479,784\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 12\n",
      "Author Name: Twilight\n",
      "Volumes Sold: Meyer, Stephenie\n",
      "Publisher: 2,315,405\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book Name: 13\n",
      "Author Name: Girl with the Dragon Tattoo,The:Millennium Trilogy\n",
      "Volumes Sold: Larsson, Stieg\n",
      "Publisher: 2,233,570\n",
      "Genre: Quercus\n",
      "\n",
      "Book Name: 14\n",
      "Author Name: Fifty Shades Freed\n",
      "Volumes Sold: James, E. L.\n",
      "Publisher: 2,193,928\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 15\n",
      "Author Name: Lost Symbol,The\n",
      "Volumes Sold: Brown, Dan\n",
      "Publisher: 2,183,031\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 16\n",
      "Author Name: New Moon\n",
      "Volumes Sold: Meyer, Stephenie\n",
      "Publisher: 2,152,737\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book Name: 17\n",
      "Author Name: Deception Point\n",
      "Volumes Sold: Brown, Dan\n",
      "Publisher: 2,062,145\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 18\n",
      "Author Name: Eclipse\n",
      "Volumes Sold: Meyer, Stephenie\n",
      "Publisher: 2,052,876\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book Name: 19\n",
      "Author Name: Lovely Bones,The\n",
      "Volumes Sold: Sebold, Alice\n",
      "Publisher: 2,005,598\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book Name: 20\n",
      "Author Name: Curious Incident of the Dog in the Night-time,The\n",
      "Volumes Sold: Haddon, Mark\n",
      "Publisher: 1,979,552\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 21\n",
      "Author Name: Digital Fortress\n",
      "Volumes Sold: Brown, Dan\n",
      "Publisher: 1,928,900\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 22\n",
      "Author Name: Short History of Nearly Everything,A\n",
      "Volumes Sold: Bryson, Bill\n",
      "Publisher: 1,852,919\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 23\n",
      "Author Name: Girl Who Played with Fire,The:Millennium Trilogy\n",
      "Volumes Sold: Larsson, Stieg\n",
      "Publisher: 1,814,784\n",
      "Genre: Quercus\n",
      "\n",
      "Book Name: 24\n",
      "Author Name: Breaking Dawn\n",
      "Volumes Sold: Meyer, Stephenie\n",
      "Publisher: 1,787,118\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book Name: 25\n",
      "Author Name: Very Hungry Caterpillar,The:The Very Hungry Caterpillar\n",
      "Volumes Sold: Carle, Eric\n",
      "Publisher: 1,783,535\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 26\n",
      "Author Name: Gruffalo,The\n",
      "Volumes Sold: Donaldson, Julia\n",
      "Publisher: 1,781,269\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book Name: 27\n",
      "Author Name: Jamie's 30-Minute Meals\n",
      "Volumes Sold: Oliver, Jamie\n",
      "Publisher: 1,743,266\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 28\n",
      "Author Name: Kite Runner,The\n",
      "Volumes Sold: Hosseini, Khaled\n",
      "Publisher: 1,629,119\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 29\n",
      "Author Name: One Day\n",
      "Volumes Sold: Nicholls, David\n",
      "Publisher: 1,616,068\n",
      "Genre: Hodder & Stoughton\n",
      "\n",
      "Book Name: 30\n",
      "Author Name: Thousand Splendid Suns,A\n",
      "Volumes Sold: Hosseini, Khaled\n",
      "Publisher: 1,583,992\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 31\n",
      "Author Name: Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy\n",
      "Volumes Sold: Larsson, Stieg\n",
      "Publisher: 1,555,135\n",
      "Genre: Quercus\n",
      "\n",
      "Book Name: 32\n",
      "Author Name: Time Traveler's Wife,The\n",
      "Volumes Sold: Niffenegger, Audrey\n",
      "Publisher: 1,546,886\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 33\n",
      "Author Name: Atonement\n",
      "Volumes Sold: McEwan, Ian\n",
      "Publisher: 1,539,428\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 34\n",
      "Author Name: Bridget Jones's Diary:A Novel\n",
      "Volumes Sold: Fielding, Helen\n",
      "Publisher: 1,508,205\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book Name: 35\n",
      "Author Name: World According to Clarkson,The\n",
      "Volumes Sold: Clarkson, Jeremy\n",
      "Publisher: 1,489,403\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 36\n",
      "Author Name: Captain Corelli's Mandolin\n",
      "Volumes Sold: Bernieres, Louis de\n",
      "Publisher: 1,352,318\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 37\n",
      "Author Name: Sound of Laughter,The\n",
      "Volumes Sold: Kay, Peter\n",
      "Publisher: 1,310,207\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 38\n",
      "Author Name: Life of Pi\n",
      "Volumes Sold: Martel, Yann\n",
      "Publisher: 1,310,176\n",
      "Genre: Canongate\n",
      "\n",
      "Book Name: 39\n",
      "Author Name: Billy Connolly\n",
      "Volumes Sold: Stephenson, Pamela\n",
      "Publisher: 1,231,957\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book Name: 40\n",
      "Author Name: Child Called It,A\n",
      "Volumes Sold: Pelzer, Dave\n",
      "Publisher: 1,217,712\n",
      "Genre: Orion\n",
      "\n",
      "Book Name: 41\n",
      "Author Name: Gruffalo's Child,The\n",
      "Volumes Sold: Donaldson, Julia\n",
      "Publisher: 1,208,711\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book Name: 42\n",
      "Author Name: Angela's Ashes:A Memoir of a Childhood\n",
      "Volumes Sold: McCourt, Frank\n",
      "Publisher: 1,204,058\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book Name: 43\n",
      "Author Name: Birdsong\n",
      "Volumes Sold: Faulks, Sebastian\n",
      "Publisher: 1,184,967\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 44\n",
      "Author Name: Northern Lights:His Dark Materials S.\n",
      "Volumes Sold: Pullman, Philip\n",
      "Publisher: 1,181,503\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "Book Name: 45\n",
      "Author Name: Labyrinth\n",
      "Volumes Sold: Mosse, Kate\n",
      "Publisher: 1,181,093\n",
      "Genre: Orion\n",
      "\n",
      "Book Name: 46\n",
      "Author Name: Harry Potter and the Half-blood Prince\n",
      "Volumes Sold: Rowling, J.K.\n",
      "Publisher: 1,153,181\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 47\n",
      "Author Name: Help,The\n",
      "Volumes Sold: Stockett, Kathryn\n",
      "Publisher: 1,132,336\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 48\n",
      "Author Name: Man and Boy\n",
      "Volumes Sold: Parsons, Tony\n",
      "Publisher: 1,130,802\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book Name: 49\n",
      "Author Name: Memoirs of a Geisha\n",
      "Volumes Sold: Golden, Arthur\n",
      "Publisher: 1,126,337\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 50\n",
      "Author Name: No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S.\n",
      "Volumes Sold: McCall Smith, Alexander\n",
      "Publisher: 1,115,549\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book Name: 51\n",
      "Author Name: Island,The\n",
      "Volumes Sold: Hislop, Victoria\n",
      "Publisher: 1,108,328\n",
      "Genre: Headline\n",
      "\n",
      "Book Name: 52\n",
      "Author Name: PS, I Love You\n",
      "Volumes Sold: Ahern, Cecelia\n",
      "Publisher: 1,107,379\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book Name: 53\n",
      "Author Name: You are What You Eat:The Plan That Will Change Your Life\n",
      "Volumes Sold: McKeith, Gillian\n",
      "Publisher: 1,104,403\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 54\n",
      "Author Name: Shadow of the Wind,The\n",
      "Volumes Sold: Zafon, Carlos Ruiz\n",
      "Publisher: 1,092,349\n",
      "Genre: Orion\n",
      "\n",
      "Book Name: 55\n",
      "Author Name: Tales of Beedle the Bard,The\n",
      "Volumes Sold: Rowling, J.K.\n",
      "Publisher: 1,090,847\n",
      "Genre: Bloomsbury\n",
      "\n",
      "Book Name: 56\n",
      "Author Name: Broker,The\n",
      "Volumes Sold: Grisham, John\n",
      "Publisher: 1,087,262\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 57\n",
      "Author Name: Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P\n",
      "Volumes Sold: Atkins, Robert C.\n",
      "Publisher: 1,054,196\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 58\n",
      "Author Name: Subtle Knife,The:His Dark Materials S.\n",
      "Volumes Sold: Pullman, Philip\n",
      "Publisher: 1,037,160\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "Book Name: 59\n",
      "Author Name: Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation\n",
      "Volumes Sold: Truss, Lynne\n",
      "Publisher: 1,023,688\n",
      "Genre: Profile Books Group\n",
      "\n",
      "Book Name: 60\n",
      "Author Name: Delia's How to Cook:(Bk.1)\n",
      "Volumes Sold: Smith, Delia\n",
      "Publisher: 1,015,956\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 61\n",
      "Author Name: Chocolat\n",
      "Volumes Sold: Harris, Joanne\n",
      "Publisher: 1,009,873\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 62\n",
      "Author Name: Boy in the Striped Pyjamas,The\n",
      "Volumes Sold: Boyne, John\n",
      "Publisher: 1,004,414\n",
      "Genre: Random House Childrens Books G\n",
      "\n",
      "Book Name: 63\n",
      "Author Name: My Sister's Keeper\n",
      "Volumes Sold: Picoult, Jodi\n",
      "Publisher: 1,003,780\n",
      "Genre: Hodder & Stoughton\n",
      "\n",
      "Book Name: 64\n",
      "Author Name: Amber Spyglass,The:His Dark Materials S.\n",
      "Volumes Sold: Pullman, Philip\n",
      "Publisher: 1,002,314\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "Book Name: 65\n",
      "Author Name: To Kill a Mockingbird\n",
      "Volumes Sold: Lee, Harper\n",
      "Publisher: 998,213\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 66\n",
      "Author Name: Men are from Mars, Women are from Venus:A Practical Guide for Improvin\n",
      "Volumes Sold: Gray, John\n",
      "Publisher: 992,846\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book Name: 67\n",
      "Author Name: Dear Fatty\n",
      "Volumes Sold: French, Dawn\n",
      "Publisher: 986,753\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 68\n",
      "Author Name: Short History of Tractors in Ukrainian,A\n",
      "Volumes Sold: Lewycka, Marina\n",
      "Publisher: 986,115\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 69\n",
      "Author Name: Hannibal\n",
      "Volumes Sold: Harris, Thomas\n",
      "Publisher: 970,509\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 70\n",
      "Author Name: Lord of the Rings,The\n",
      "Volumes Sold: Tolkien, J. R. R.\n",
      "Publisher: 967,466\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book Name: 71\n",
      "Author Name: Stupid White Men:...and Other Sorry Excuses for the State of the Natio\n",
      "Volumes Sold: Moore, Michael\n",
      "Publisher: 963,353\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 72\n",
      "Author Name: Interpretation of Murder,The\n",
      "Volumes Sold: Rubenfeld, Jed\n",
      "Publisher: 962,515\n",
      "Genre: Headline\n",
      "\n",
      "Book Name: 73\n",
      "Author Name: Sharon Osbourne Extreme:My Autobiography\n",
      "Volumes Sold: Osbourne, Sharon\n",
      "Publisher: 959,496\n",
      "Genre: Little, Brown Book\n",
      "\n",
      "Book Name: 74\n",
      "Author Name: Alchemist,The:A Fable About Following Your Dream\n",
      "Volumes Sold: Coelho, Paulo\n",
      "Publisher: 956,114\n",
      "Genre: HarperCollins\n",
      "\n",
      "Book Name: 75\n",
      "Author Name: At My Mother's Knee ...:and Other Low Joints\n",
      "Volumes Sold: O'Grady, Paul\n",
      "Publisher: 945,640\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 76\n",
      "Author Name: Notes from a Small Island\n",
      "Volumes Sold: Bryson, Bill\n",
      "Publisher: 931,312\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 77\n",
      "Author Name: Return of the Naked Chef,The\n",
      "Volumes Sold: Oliver, Jamie\n",
      "Publisher: 925,425\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 78\n",
      "Author Name: Bridget Jones: The Edge of Reason\n",
      "Volumes Sold: Fielding, Helen\n",
      "Publisher: 924,695\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book Name: 79\n",
      "Author Name: Jamie's Italy\n",
      "Volumes Sold: Oliver, Jamie\n",
      "Publisher: 906,968\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 80\n",
      "Author Name: I Can Make You Thin\n",
      "Volumes Sold: McKenna, Paul\n",
      "Publisher: 905,086\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 81\n",
      "Author Name: Down Under\n",
      "Volumes Sold: Bryson, Bill\n",
      "Publisher: 890,847\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 82\n",
      "Author Name: Summons,The\n",
      "Volumes Sold: Grisham, John\n",
      "Publisher: 869,671\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 83\n",
      "Author Name: Small Island\n",
      "Volumes Sold: Levy, Andrea\n",
      "Publisher: 869,659\n",
      "Genre: Headline\n",
      "\n",
      "Book Name: 84\n",
      "Author Name: Nigella Express\n",
      "Volumes Sold: Lawson, Nigella\n",
      "Publisher: 862,602\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 85\n",
      "Author Name: Brick Lane\n",
      "Volumes Sold: Ali, Monica\n",
      "Publisher: 856,540\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 86\n",
      "Author Name: Memory Keeper's Daughter,The\n",
      "Volumes Sold: Edwards, Kim\n",
      "Publisher: 845,858\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 87\n",
      "Author Name: Room on the Broom\n",
      "Volumes Sold: Donaldson, Julia\n",
      "Publisher: 842,535\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book Name: 88\n",
      "Author Name: About a Boy\n",
      "Volumes Sold: Hornby, Nick\n",
      "Publisher: 828,215\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 89\n",
      "Author Name: My Booky Wook\n",
      "Volumes Sold: Brand, Russell\n",
      "Publisher: 820,563\n",
      "Genre: Hodder & Stoughton\n",
      "\n",
      "Book Name: 90\n",
      "Author Name: God Delusion,The\n",
      "Volumes Sold: Dawkins, Richard\n",
      "Publisher: 816,907\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 91\n",
      "Author Name: \"Beano\" Annual,The\n",
      "Volumes Sold: 0\n",
      "Publisher: 816,585\n",
      "Genre: D.C. Thomson\n",
      "\n",
      "Book Name: 92\n",
      "Author Name: White Teeth\n",
      "Volumes Sold: Smith, Zadie\n",
      "Publisher: 815,586\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 93\n",
      "Author Name: House at Riverton,The\n",
      "Volumes Sold: Morton, Kate\n",
      "Publisher: 814,370\n",
      "Genre: Pan Macmillan\n",
      "\n",
      "Book Name: 94\n",
      "Author Name: Book Thief,The\n",
      "Volumes Sold: Zusak, Markus\n",
      "Publisher: 809,641\n",
      "Genre: Transworld\n",
      "\n",
      "Book Name: 95\n",
      "Author Name: Nights of Rain and Stars\n",
      "Volumes Sold: Binchy, Maeve\n",
      "Publisher: 808,900\n",
      "Genre: Orion\n",
      "\n",
      "Book Name: 96\n",
      "Author Name: Ghost,The\n",
      "Volumes Sold: Harris, Robert\n",
      "Publisher: 807,311\n",
      "Genre: Random House\n",
      "\n",
      "Book Name: 97\n",
      "Author Name: Happy Days with the Naked Chef\n",
      "Volumes Sold: Oliver, Jamie\n",
      "Publisher: 794,201\n",
      "Genre: Penguin\n",
      "\n",
      "Book Name: 98\n",
      "Author Name: Hunger Games,The:Hunger Games Trilogy\n",
      "Volumes Sold: Collins, Suzanne\n",
      "Publisher: 792,187\n",
      "Genre: Scholastic Ltd.\n",
      "\n",
      "Book Name: 99\n",
      "Author Name: Lost Boy,The:A Foster Child's Search for the Love of a Family\n",
      "Volumes Sold: Pelzer, Dave\n",
      "Publisher: 791,507\n",
      "Genre: Orion\n",
      "\n",
      "Book Name: 100\n",
      "Author Name: Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours\n",
      "Volumes Sold: Oliver, Jamie\n",
      "Publisher: 791,095\n",
      "Genre: Penguin\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of The Guardian's page for highest selling novels\n",
    "url = \"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the table containing the highest selling novels data\n",
    "table = soup.find('table', class_='in-article sortable')\n",
    "\n",
    "# Initialize lists to store the details\n",
    "book_name_list = []\n",
    "author_name_list = []\n",
    "volumes_sold_list = []\n",
    "publisher_list = []\n",
    "genre_list = []\n",
    "\n",
    "# Extract data from each row in the table\n",
    "rows = table.find_all('tr')\n",
    "for row in rows[1:]:\n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) >= 5:  # Check if the row has at least 5 columns\n",
    "        book_name = columns[0].text.strip()\n",
    "        author_name = columns[1].text.strip()\n",
    "        volumes_sold = columns[2].text.strip()\n",
    "        publisher = columns[3].text.strip()\n",
    "        genre = columns[4].text.strip()\n",
    "        \n",
    "        # Append data to respective lists\n",
    "        book_name_list.append(book_name)\n",
    "        author_name_list.append(author_name)\n",
    "        volumes_sold_list.append(volumes_sold)\n",
    "        publisher_list.append(publisher)\n",
    "        genre_list.append(genre)\n",
    "\n",
    "# Print the details of the highest selling novels\n",
    "for i in range(len(book_name_list)):\n",
    "    print(\"Book Name:\", book_name_list[i])\n",
    "    print(\"Author Name:\", author_name_list[i])\n",
    "    print(\"Volumes Sold:\", volumes_sold_list[i])\n",
    "    print(\"Publisher:\", publisher_list[i])\n",
    "    print(\"Genre:\", genre_list[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0601fe46-2acb-4e95-a85b-39dbc4aee1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "086ebfdc-f81a-402f-96e3-280e3808437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the IMDb page for most watched TV series of all time\n",
    "url = \"https://www.imdb.com/list/ls095964455/\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "# Find the list of TV series\n",
    "series_list = soup.find_all('div', class_='lister-item-content')\n",
    "\n",
    "# Initialize lists to store the details\n",
    "name_list = []\n",
    "year_span_list = []\n",
    "genre_list = []\n",
    "runtime_list = []\n",
    "ratings_list = []\n",
    "votes_list = []\n",
    "\n",
    "# Extract data from each TV series\n",
    "for series in series_list:\n",
    "    name_elem = series.find('h3', class_='lister-item-header').a\n",
    "    if name_elem:\n",
    "        name = name_elem.text.strip()\n",
    "    else:\n",
    "        name = \"N/A\"\n",
    "    \n",
    "    year_span_elem = series.find('span', class_='lister-item-year')\n",
    "    if year_span_elem:\n",
    "        year_span = year_span_elem.text.strip()\n",
    "    else:\n",
    "        year_span = \"N/A\"\n",
    "    \n",
    "    genre_elem = series.find('span', class_='genre')\n",
    "    if genre_elem:\n",
    "        genre = genre_elem.text.strip()\n",
    "    else:\n",
    "        genre = \"N/A\"\n",
    "    \n",
    "    runtime_elem = series.find('span', class_='runtime')\n",
    "    if runtime_elem:\n",
    "        runtime = runtime_elem.text.strip()\n",
    "    else:\n",
    "        runtime = \"N/A\"\n",
    "    \n",
    "    ratings_elem = series.find('span', class_='ipl-rating-star__rating')\n",
    "    if ratings_elem:\n",
    "        ratings = ratings_elem.text.strip()\n",
    "    else:\n",
    "        ratings = \"N/A\"\n",
    "    \n",
    "    votes_elem = series.find('span', attrs={'name': 'nv'})\n",
    "    if votes_elem:\n",
    "        votes = votes_elem['data-value']\n",
    "    else:\n",
    "        votes = \"N/A\"\n",
    "    \n",
    "    # Append data to respective lists\n",
    "    name_list.append(name)\n",
    "    year_span_list.append(year_span)\n",
    "    genre_list.append(genre)\n",
    "    runtime_list.append(runtime)\n",
    "    ratings_list.append(ratings)\n",
    "    votes_list.append(votes)\n",
    "\n",
    "# Print the details of the most watched TV series\n",
    "for i in range(len(name_list)):\n",
    "    print(\"Name:\", name_list[i])\n",
    "    print(\"Year Span:\", year_span_list[i])\n",
    "    print(\"Genre:\", genre_list[i])\n",
    "    print(\"Run Time:\", runtime_list[i])\n",
    "    print(\"Ratings:\", ratings_list[i])\n",
    "    print(\"Votes:\", votes_list[i])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11ae0a02-bfbd-4a03-a679-8e27c3363f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "074e9ff7-273d-4dc7-bcd5-0d42bfdedc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link to 'Show All Dataset' page not found on the homepage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the UCI Machine Learning Repository homepage\n",
    "homepage_url = \"https://archive.ics.uci.edu/\"\n",
    "\n",
    "# Send a GET request to the homepage URL\n",
    "homepage_response = requests.get(homepage_url)\n",
    "\n",
    "# Parse the homepage HTML content\n",
    "homepage_soup = BeautifulSoup(homepage_response.text, 'html.parser')\n",
    "\n",
    "# Find the link to the \"Show All Dataset\" page\n",
    "all_datasets_link = homepage_soup.find('a', href='/ml/datasets.php')\n",
    "\n",
    "if all_datasets_link:\n",
    "    all_datasets_url = homepage_url + all_datasets_link['href']\n",
    "    \n",
    "    # Send a GET request to the \"Show All Dataset\" page\n",
    "    all_datasets_response = requests.get(all_datasets_url)\n",
    "    \n",
    "    # Parse the \"Show All Dataset\" page HTML content\n",
    "    all_datasets_soup = BeautifulSoup(all_datasets_response.text, 'html.parser')\n",
    "    \n",
    "    # Find the table containing the dataset details\n",
    "    table = all_datasets_soup.find('table', class_='list')\n",
    "    \n",
    "    # Initialize lists to store the details\n",
    "    dataset_name_list = []\n",
    "    data_type_list = []\n",
    "    task_list = []\n",
    "    attribute_type_list = []\n",
    "    instances_list = []\n",
    "    attribute_count_list = []\n",
    "    year_list = []\n",
    "    \n",
    "    # Extract data from each row in the table\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        dataset_name = columns[0].text.strip()\n",
    "        data_type = columns[1].text.strip()\n",
    "        task = columns[2].text.strip()\n",
    "        attribute_type = columns[3].text.strip()\n",
    "        instances = columns[4].text.strip()\n",
    "        attribute_count = columns[5].text.strip()\n",
    "        year = columns[6].text.strip()\n",
    "        \n",
    "        # Append data to respective lists\n",
    "        dataset_name_list.append(dataset_name)\n",
    "        data_type_list.append(data_type)\n",
    "        task_list.append(task)\n",
    "        attribute_type_list.append(attribute_type)\n",
    "        instances_list.append(instances)\n",
    "        attribute_count_list.append(attribute_count)\n",
    "        year_list.append(year)\n",
    "    \n",
    "    # Print the details of datasets\n",
    "    for i in range(len(dataset_name_list)):\n",
    "        print(\"Dataset Name:\", dataset_name_list[i])\n",
    "        print(\"Data Type:\", data_type_list[i])\n",
    "        print(\"Task:\", task_list[i])\n",
    "        print(\"Attribute Type:\", attribute_type_list[i])\n",
    "        print(\"No of Instances:\", instances_list[i])\n",
    "        print(\"No of Attributes:\", attribute_count_list[i])\n",
    "        print(\"Year:\", year_list[i])\n",
    "        print()\n",
    "else:\n",
    "    print(\"Link to 'Show All Dataset' page not found on the homepage.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633419ed-1a6b-4e99-a743-f7aed29c90c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
