{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7f3cb1b-5d7c-4b76-b42f-b110a8f5816e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad71ac23-1861-424d-9f41-159943ae0dbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://www.shine.com/api/search/v1/search",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m\n\u001b[0;32m     58\u001b[0m job_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Analyst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBangalore\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 60\u001b[0m df_jobs \u001b[38;5;241m=\u001b[39m scrape_jobs_data(job_title, location)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_jobs)\n",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m, in \u001b[0;36mscrape_jobs_data\u001b[1;34m(job_title, location, num_jobs)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Send a POST request with the search parameters\u001b[39;00m\n\u001b[0;32m     21\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.shine.com/api/search/v1/search\u001b[39m\u001b[38;5;124m\"\u001b[39m, data\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m---> 22\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for unsuccessful status codes\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Parse the JSON response\u001b[39;00m\n\u001b[0;32m     25\u001b[0m data \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()\n",
      "File \u001b[1;32mE:\\Data Science\\Anaconda\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://www.shine.com/api/search/v1/search"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_jobs_data(job_title, location, num_jobs=10):\n",
    "    # Set up search parameters\n",
    "    search_url = \"https://www.shine.com/job-search/search/jobs-in-bangalore?loc=Bangalore&q=Data%20Analyst\"\n",
    "    params = {\n",
    "        'l': location,\n",
    "        'searchUrl': search_url,\n",
    "        'searchType': 'job',\n",
    "        'from': 'homepagedeepinsights',\n",
    "        'productCode': 'homepage',\n",
    "        'searchId': 'homepagedeepinsights',\n",
    "        'sk': 'sjs',\n",
    "        'key': job_title,\n",
    "        'src': 'refine'\n",
    "    }\n",
    "    \n",
    "    # Send a POST request with the search parameters\n",
    "    response = requests.post(\"https://www.shine.com/api/search/v1/search\", data=params)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    data = response.json()\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "    \n",
    "    # Extract details for each job listing\n",
    "    for listing in data['searchResults']:\n",
    "        # Extract job title\n",
    "        job_titles.append(listing['jobTitle'])\n",
    "        \n",
    "        # Extract job location\n",
    "        job_locations.append(listing['location'])\n",
    "        \n",
    "        # Extract company name\n",
    "        company_names.append(listing['companyName'])\n",
    "        \n",
    "        # Extract experience required\n",
    "        experience_required.append(listing['experience'])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Job Title': job_titles,\n",
    "        'Job Location': job_locations,\n",
    "        'Company Name': company_names,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Scrape jobs data for \"Data Analyst\" position in \"Bangalore\" location\n",
    "job_title = \"Data Analyst\"\n",
    "location = \"Bangalore\"\n",
    "df_jobs = scrape_jobs_data(job_title, location)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e7b85dc-d2a8-4912-9c34-9378555c1762",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5fc609c-ce2e-42fd-a5bb-29a0a54dfc92",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://www.shine.com/job-search/search/p?q=Data+Scientist&l=Bangalore&p=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 54\u001b[0m\n\u001b[0;32m     52\u001b[0m job_title \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData Scientist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     53\u001b[0m location \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBangalore\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 54\u001b[0m df_jobs \u001b[38;5;241m=\u001b[39m scrape_jobs_data(job_title, location)\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_jobs)\n",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m, in \u001b[0;36mscrape_jobs_data\u001b[1;34m(job_title, location, num_jobs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Send a POST request with the search parameters\u001b[39;00m\n\u001b[0;32m     14\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.shine.com/job-search/search/p\u001b[39m\u001b[38;5;124m\"\u001b[39m, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m---> 15\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for unsuccessful status codes\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Parse the HTML content of the page\u001b[39;00m\n\u001b[0;32m     18\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Data Science\\Anaconda\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://www.shine.com/job-search/search/p?q=Data+Scientist&l=Bangalore&p=1"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_jobs_data(job_title, location, num_jobs=10):\n",
    "    # Set up search parameters\n",
    "    params = {\n",
    "        'q': job_title,\n",
    "        'l': location,\n",
    "        'p': 1  # Start page\n",
    "    }\n",
    "    \n",
    "    # Send a POST request with the search parameters\n",
    "    response = requests.get(\"https://www.shine.com/job-search/search/p\", params=params)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "    \n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all job listings\n",
    "    job_listings = soup.find_all('div', class_='result-display')\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    \n",
    "    # Extract details for each job listing\n",
    "    for listing in job_listings[:num_jobs]:\n",
    "        # Extract job title\n",
    "        job_title = listing.find('a', class_='job_title_anchor').text.strip()\n",
    "        job_titles.append(job_title)\n",
    "        \n",
    "        # Extract job location\n",
    "        job_location = listing.find('li', class_='w-30 mr-10').text.strip()\n",
    "        job_locations.append(job_location)\n",
    "        \n",
    "        # Extract company name\n",
    "        company_name = listing.find('span', class_='result-display__profile__company').text.strip()\n",
    "        company_names.append(company_name)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Job Title': job_titles,\n",
    "        'Job Location': job_locations,\n",
    "        'Company Name': company_names\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Scrape jobs data for \"Data Scientist\" position in \"Bangalore\" location\n",
    "job_title = \"Data Scientist\"\n",
    "location = \"Bangalore\"\n",
    "df_jobs = scrape_jobs_data(job_title, location)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e814d771-374a-4761-b61c-9bfb3c195755",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa7a132d-9697-45af-bae4-8dbbeba3f55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Job Title, Job Location, Company Name, Experience Required]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_jobs_data(job_title, location, salary, num_jobs=10):\n",
    "    # Set up search parameters\n",
    "    params = {\n",
    "        'q': job_title,\n",
    "        'l': location,\n",
    "        's': salary,\n",
    "        'p': 1  # Start page\n",
    "    }\n",
    "    \n",
    "    # Send a POST request with the search parameters\n",
    "    response = requests.post(\"https://www.shine.com/job-search/search\", params=params)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "    \n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    # Find all job listings\n",
    "    job_listings = soup.find_all('li', class_='sjs-result')\n",
    "    \n",
    "    # Initialize lists to store data\n",
    "    job_titles = []\n",
    "    job_locations = []\n",
    "    company_names = []\n",
    "    experience_required = []\n",
    "    \n",
    "    # Extract details for each job listing\n",
    "    for listing in job_listings[:num_jobs]:\n",
    "        # Extract job title\n",
    "        job_title = listing.find('a', class_='job_title_anchor').text.strip()\n",
    "        job_titles.append(job_title)\n",
    "        \n",
    "        # Extract job location\n",
    "        job_location = listing.find('li', class_='w-30 mr-10').text.strip()\n",
    "        job_locations.append(job_location)\n",
    "        \n",
    "        # Extract company name\n",
    "        company_name = listing.find('span', class_='result-display__profile__company').text.strip()\n",
    "        company_names.append(company_name)\n",
    "        \n",
    "        # Extract experience required\n",
    "        exp_required = listing.find('span', class_='sjs-exp').text.strip()\n",
    "        experience_required.append(exp_required)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Job Title': job_titles,\n",
    "        'Job Location': job_locations,\n",
    "        'Company Name': company_names,\n",
    "        'Experience Required': experience_required\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Scrape jobs data for \"Data Scientist\" designation with location filter \"Delhi/NCR\" and salary filter \"3-6 lakhs\"\n",
    "job_title = \"Data Scientist\"\n",
    "location = \"Delhi/NCR\"\n",
    "salary = \"3-6 Lakhs\"\n",
    "df_jobs = scrape_jobs_data(job_title, location, salary)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_jobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d8445cc-740a-4521-8d47-9c9225e379b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa22a252-d711-463a-96fd-e3f506391906",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://www.flipkart.com/search?q=sunglasses",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Scrape data for the first 100 sunglasses listings\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m df_sunglasses \u001b[38;5;241m=\u001b[39m scrape_sunglasses_data(num_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_sunglasses)\n",
      "Cell \u001b[1;32mIn[10], line 21\u001b[0m, in \u001b[0;36mscrape_sunglasses_data\u001b[1;34m(num_items)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(brands) \u001b[38;5;241m<\u001b[39m num_items:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Send GET request to search URL\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(search_url)\n\u001b[1;32m---> 21\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for unsuccessful status codes\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Parse HTML content\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Data Science\\Anaconda\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://www.flipkart.com/search?q=sunglasses"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_sunglasses_data(num_items=100):\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = \"https://www.flipkart.com/search?q=sunglasses\"\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    brands = []\n",
    "    descriptions = []\n",
    "    prices = []\n",
    "    ratings = []\n",
    "    review_summaries = []\n",
    "    full_reviews = []\n",
    "\n",
    "    # Scrape data from search results\n",
    "    while len(brands) < num_items:\n",
    "        # Send GET request to search URL\n",
    "        response = requests.get(search_url)\n",
    "        response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all sunglasses listings\n",
    "        items = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "        # Extract data from each listing\n",
    "        for item in items:\n",
    "            # Extract brand\n",
    "            brand = item.find('div', class_='_2WkVRV').text.strip()\n",
    "            brands.append(brand)\n",
    "\n",
    "            # Extract description\n",
    "            description = item.find('a', class_='IRpwTa').text.strip()\n",
    "            descriptions.append(description)\n",
    "\n",
    "            # Extract price\n",
    "            price = item.find('div', class_='_30jeq3').text.strip()\n",
    "            prices.append(price)\n",
    "\n",
    "            # Extract link to the product page\n",
    "            product_url = base_url + item.find('a', class_='IRpwTa')['href']\n",
    "\n",
    "            # Send GET request to the product page\n",
    "            product_response = requests.get(product_url)\n",
    "            product_response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "            # Parse HTML content of the product page\n",
    "            product_soup = BeautifulSoup(product_response.text, 'html.parser')\n",
    "\n",
    "            # Extract rating\n",
    "            rating = product_soup.find('div', class_='_3LWZlK').text.strip()\n",
    "            ratings.append(rating)\n",
    "\n",
    "            # Extract review summary\n",
    "            review_summary = product_soup.find('p', class_='_2-N8zT').text.strip()\n",
    "            review_summaries.append(review_summary)\n",
    "\n",
    "            # Extract full review\n",
    "            full_review = product_soup.find('div', class_='t-ZTKy').text.strip()\n",
    "            full_reviews.append(full_review)\n",
    "\n",
    "            # Check if we have reached the desired number of items\n",
    "            if len(brands) >= num_items:\n",
    "                break\n",
    "\n",
    "        # Check if there is a next page\n",
    "        next_button = soup.find('a', class_='_1LKTO3')\n",
    "        if next_button:\n",
    "            # Update search URL for the next page\n",
    "            search_url = base_url + next_button['href']\n",
    "        else:\n",
    "            # Exit loop if there is no next page\n",
    "            break\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Brand': brands[:num_items],\n",
    "        'Product Description': descriptions[:num_items],\n",
    "        'Price': prices[:num_items],\n",
    "        'Rating': ratings[:num_items],\n",
    "        'Review Summary': review_summaries[:num_items],\n",
    "        'Full Review': full_reviews[:num_items]\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Scrape data for the first 100 sunglasses listings\n",
    "df_sunglasses = scrape_sunglasses_data(num_items=100)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_sunglasses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56367b5e-5014-4749-b8e2-1d9f80781317",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c8db2bc-61a9-468f-b3c2-5be1374e4da4",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://www.flipkart.com/search?q=sneakers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Scrape data for the first 100 sneakers listings\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m df_sneakers \u001b[38;5;241m=\u001b[39m scrape_sneakers_data(num_items\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_sneakers)\n",
      "Cell \u001b[1;32mIn[13], line 18\u001b[0m, in \u001b[0;36mscrape_sneakers_data\u001b[1;34m(num_items)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(brands) \u001b[38;5;241m<\u001b[39m num_items:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Send GET request to search URL\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(search_url)\n\u001b[1;32m---> 18\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for unsuccessful status codes\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Parse HTML content\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Data Science\\Anaconda\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://www.flipkart.com/search?q=sneakers"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_sneakers_data(num_items=100):\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "    search_url = \"https://www.flipkart.com/search?q=sneakers\"\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    brands = []\n",
    "    descriptions = []\n",
    "    prices = []\n",
    "\n",
    "    # Scrape data from search results\n",
    "    while len(brands) < num_items:\n",
    "        # Send GET request to search URL\n",
    "        response = requests.get(search_url)\n",
    "        response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all sneakers listings\n",
    "        items = soup.find_all('div', class_='_1AtVbE')\n",
    "\n",
    "        # Extract data from each listing\n",
    "        for item in items:\n",
    "            # Extract brand\n",
    "            brand = item.find('div', class_='_2WkVRV').text.strip()\n",
    "            brands.append(brand)\n",
    "\n",
    "            # Extract description\n",
    "            description = item.find('a', class_='IRpwTa').text.strip()\n",
    "            descriptions.append(description)\n",
    "\n",
    "            # Extract price\n",
    "            price = item.find('div', class_='_30jeq3').text.strip()\n",
    "            prices.append(price)\n",
    "\n",
    "            # Check if we have reached the desired number of items\n",
    "            if len(brands) >= num_items:\n",
    "                break\n",
    "\n",
    "        # Check if there is a next page\n",
    "        next_button = soup.find('a', class_='_1LKTO3')\n",
    "        if next_button:\n",
    "            # Update search URL for the next page\n",
    "            search_url = base_url + next_button['href']\n",
    "        else:\n",
    "            # Exit loop if there is no next page\n",
    "            break\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Brand': brands[:num_items],\n",
    "        'Product Description': descriptions[:num_items],\n",
    "        'Price': prices[:num_items]\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Scrape data for the first 100 sneakers listings\n",
    "df_sneakers = scrape_sneakers_data(num_items=100)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_sneakers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1e6520-8e94-4188-98a7-62ae9cbead69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92f82c53-e739-477e-bef2-208ee42ee01b",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "500 Server Error: Internal Server Error for url: https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 65\u001b[0m\n\u001b[0;32m     62\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Scrape data for the first 100 reviews\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m df_reviews \u001b[38;5;241m=\u001b[39m scrape_reviews_data(url, num_reviews\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_reviews)\n",
      "Cell \u001b[1;32mIn[14], line 17\u001b[0m, in \u001b[0;36mscrape_reviews_data\u001b[1;34m(url, num_reviews)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ratings) \u001b[38;5;241m<\u001b[39m num_reviews:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;66;03m# Send GET request to reviews URL\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[1;32m---> 17\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for unsuccessful status codes\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Parse HTML content\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Data Science\\Anaconda\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_reviews_data(url, num_reviews=100):\n",
    "    base_url = \"https://www.flipkart.com\"\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    ratings = []\n",
    "    review_summaries = []\n",
    "    full_reviews = []\n",
    "\n",
    "    # Scrape data from reviews page\n",
    "    while len(ratings) < num_reviews:\n",
    "        # Send GET request to reviews URL\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Find all review listings\n",
    "        items = soup.find_all('div', class_='col _2wzgFH K0kLPL')\n",
    "\n",
    "        # Extract data from each review\n",
    "        for item in items:\n",
    "            # Extract rating\n",
    "            rating = item.find('div', class_='hGSR34 E_uFuv').text.strip()\n",
    "            ratings.append(rating)\n",
    "\n",
    "            # Extract review summary\n",
    "            review_summary = item.find('p', class_='_2-N8zT').text.strip()\n",
    "            review_summaries.append(review_summary)\n",
    "\n",
    "            # Extract full review\n",
    "            full_review = item.find('div', class_='qwjRop').text.strip()\n",
    "            full_reviews.append(full_review)\n",
    "\n",
    "            # Check if we have reached the desired number of reviews\n",
    "            if len(ratings) >= num_reviews:\n",
    "                break\n",
    "\n",
    "        # Check if there is a next page\n",
    "        next_button = soup.find('a', class_='_1LKTO3')\n",
    "        if next_button:\n",
    "            # Update URL for the next page\n",
    "            url = base_url + next_button['href']\n",
    "        else:\n",
    "            # Exit loop if there is no next page\n",
    "            break\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Rating': ratings[:num_reviews],\n",
    "        'Review Summary': review_summaries[:num_reviews],\n",
    "        'Full Review': full_reviews[:num_reviews]\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# URL of the product reviews page for iPhone 11\n",
    "url = \"https://www.flipkart.com/apple-iphone-11-black-64-gb/product-reviews/itm4e5041ba101fd?pid=MOBFWQ6BXGJCEYNY&lid=LSTMOBFWQ6BXGJCEYNYZXSHRJ&marketplace=FLIPKART\"\n",
    "\n",
    "# Scrape data for the first 100 reviews\n",
    "df_reviews = scrape_reviews_data(url, num_reviews=100)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e4d7dd71-5dbe-409a-95e4-478aa89207e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01ace225-8250-45ab-9912-4260784b13e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "503 Server Error: Service Unavailable for url: https://www.amazon.in/s?k=Laptop&rh=n%253A976392031%252Cp_n_feature_thirteen_browse-bin%253A12598163031&dc=n&qid=1649294738&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Scrape data for the first 10 laptops with CPU Type filter set to \"Intel Core i7\"\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m df_laptops \u001b[38;5;241m=\u001b[39m scrape_laptops_data()\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_laptops)\n",
      "Cell \u001b[1;32mIn[16], line 21\u001b[0m, in \u001b[0;36mscrape_laptops_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Send GET request with search parameters\u001b[39;00m\n\u001b[0;32m     20\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(search_url, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m---> 21\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for unsuccessful status codes\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Parse HTML content\u001b[39;00m\n\u001b[0;32m     24\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Data Science\\Anaconda\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://www.amazon.in/s?k=Laptop&rh=n%253A976392031%252Cp_n_feature_thirteen_browse-bin%253A12598163031&dc=n&qid=1649294738&rnid=12598141031&ref=sr_nr_p_n_feature_thirteen_browse-bin_1"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_laptops_data():\n",
    "    base_url = \"https://www.amazon.in\"\n",
    "    search_url = \"https://www.amazon.in/s\"\n",
    "\n",
    "    # Set up search parameters\n",
    "    params = {\n",
    "        \"k\": \"Laptop\",\n",
    "        \"rh\": \"n%3A976392031%2Cp_n_feature_thirteen_browse-bin%3A12598163031\",\n",
    "        \"dc\": \"n\",\n",
    "        \"qid\": \"1649294738\",\n",
    "        \"rnid\": \"12598141031\",\n",
    "        \"ref\": \"sr_nr_p_n_feature_thirteen_browse-bin_1\"\n",
    "    }\n",
    "\n",
    "    # Send GET request with search parameters\n",
    "    response = requests.get(search_url, params=params)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all laptop listings\n",
    "    items = soup.find_all('div', class_='s-include-content-margin s-border-bottom s-latency-cf-section')\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    titles = []\n",
    "    ratings = []\n",
    "    prices = []\n",
    "\n",
    "    # Extract data for each laptop listing\n",
    "    for item in items[:10]:\n",
    "        # Extract title\n",
    "        title = item.find('span', class_='a-size-medium a-color-base a-text-normal').text.strip()\n",
    "        titles.append(title)\n",
    "\n",
    "        # Extract rating\n",
    "        rating_tag = item.find('span', class_='a-icon-alt')\n",
    "        rating = rating_tag.text.strip() if rating_tag else 'Not available'\n",
    "        ratings.append(rating)\n",
    "\n",
    "        # Extract price\n",
    "        price_tag = item.find('span', class_='a-price-whole')\n",
    "        price = price_tag.text.strip() if price_tag else 'Not available'\n",
    "        prices.append(price)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Ratings': ratings,\n",
    "        'Price': prices\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Scrape data for the first 10 laptops with CPU Type filter set to \"Intel Core i7\"\n",
    "df_laptops = scrape_laptops_data()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_laptops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13cd557e-a03a-46b4-a208-bb31f455fd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e0b6afd-c54a-455d-b435-3c1a6f01964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_2292\\528688403.py:16: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  top_quotes_link = soup.find(\"a\", text=\"Top Quotes\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Quote, Author, Type Of Quote]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape data for top 1000 quotes\n",
    "def scrape_top_quotes():\n",
    "    # Send a GET request to the webpage\n",
    "    url = \"https://www.azquotes.com/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the Top Quotes link\n",
    "    top_quotes_link = soup.find(\"a\", text=\"Top Quotes\")\n",
    "\n",
    "    # Get the URL of the Top Quotes page\n",
    "    top_quotes_url = \"https://www.azquotes.com\" + top_quotes_link[\"href\"]\n",
    "\n",
    "    # Send a GET request to the Top Quotes page\n",
    "    response = requests.get(top_quotes_url)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    # Parse the HTML content of the Top Quotes page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find all quote items\n",
    "    quote_items = soup.find_all(\"div\", class_=\"quote\")\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    quotes = []\n",
    "    authors = []\n",
    "    types = []\n",
    "\n",
    "    # Extract data for each quote\n",
    "    for item in quote_items:\n",
    "        # Extract quote\n",
    "        quote = item.find(\"a\", class_=\"title\").text.strip()\n",
    "        quotes.append(quote)\n",
    "\n",
    "        # Extract author\n",
    "        author = item.find(\"a\", class_=\"author\").text.strip()\n",
    "        authors.append(author)\n",
    "\n",
    "        # Extract type of quote\n",
    "        quote_type = item.find(\"a\", class_=\"tag\").text.strip()\n",
    "        types.append(quote_type)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Quote\": quotes,\n",
    "        \"Author\": authors,\n",
    "        \"Type Of Quote\": types\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Scrape data for top 1000 quotes\n",
    "df_quotes = scrape_top_quotes()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_quotes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cd262e7b-b028-47e3-be78-bd497c28e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4316879f-d68f-4b3e-8a94-45258827556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_2292\\1544394557.py:15: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  gk_link = soup.find(\"a\", text=\"GK\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Prime Ministers link not found\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_2292\\1544394557.py:25: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  prime_ministers_link = gk_soup.find(\"a\", text=\"List of all Prime Ministers of India\")\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_prime_ministers_data():\n",
    "    # Send a GET request to the Jagran Josh website\n",
    "    url = \"https://www.jagranjosh.com/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Click on the GK option\n",
    "    gk_link = soup.find(\"a\", text=\"GK\")\n",
    "    if not gk_link:\n",
    "        print(\"GK link not found\")\n",
    "        return None\n",
    "\n",
    "    gk_url = gk_link['href']\n",
    "    gk_response = requests.get(gk_url)\n",
    "    gk_soup = BeautifulSoup(gk_response.text, \"html.parser\")\n",
    "\n",
    "    # Click on the List of all Prime Ministers of India\n",
    "    prime_ministers_link = gk_soup.find(\"a\", text=\"List of all Prime Ministers of India\")\n",
    "    if not prime_ministers_link:\n",
    "        print(\"List of Prime Ministers link not found\")\n",
    "        return None\n",
    "\n",
    "    prime_ministers_url = prime_ministers_link['href']\n",
    "    prime_ministers_response = requests.get(prime_ministers_url)\n",
    "    prime_ministers_soup = BeautifulSoup(prime_ministers_response.text, \"html.parser\")\n",
    "\n",
    "    # Find the table containing the data\n",
    "    table = prime_ministers_soup.find(\"table\", class_=\"styled\")\n",
    "    if not table:\n",
    "        print(\"Table not found\")\n",
    "        return None\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    names = []\n",
    "    born_dead = []\n",
    "    term_of_office = []\n",
    "    remarks = []\n",
    "\n",
    "    # Extract data from the table rows\n",
    "    rows = table.find_all(\"tr\")[1:]  # Exclude header row\n",
    "    for row in rows:\n",
    "        data = row.find_all(\"td\")\n",
    "        names.append(data[0].text.strip())\n",
    "        born_dead.append(data[1].text.strip())\n",
    "        term_of_office.append(data[2].text.strip())\n",
    "        remarks.append(data[3].text.strip())\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Name\": names,\n",
    "        \"Born-Dead\": born_dead,\n",
    "        \"Term of Office\": term_of_office,\n",
    "        \"Remarks\": remarks\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Scrape data for respected former Prime Ministers of India\n",
    "df_prime_ministers = scrape_prime_ministers_data()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_prime_ministers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "54a04ae1-49b7-4b30-a51e-5552a1c9ccbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4e388c9b-9c24-493a-b304-2b33bb0ec238",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data-search'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Scrape data for the 50 most expensive cars in the world\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m df_expensive_cars \u001b[38;5;241m=\u001b[39m scrape_expensive_cars()\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Display the DataFrame\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_expensive_cars)\n",
      "Cell \u001b[1;32mIn[27], line 25\u001b[0m, in \u001b[0;36mscrape_expensive_cars\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Submit the search form\u001b[39;00m\n\u001b[0;32m     24\u001b[0m form \u001b[38;5;241m=\u001b[39m search_bar\u001b[38;5;241m.\u001b[39mfind_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mform\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(url, data\u001b[38;5;241m=\u001b[39mform\u001b[38;5;241m.\u001b[39mattrs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata-search\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     26\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for unsuccessful status codes\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Parse the HTML content of the search results page\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data-search'"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_expensive_cars():\n",
    "    # Send a GET request to the Motor1 website\n",
    "    url = \"https://www.motor1.com/\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the search bar\n",
    "    search_bar = soup.find(\"input\", attrs={\"name\": \"q\"})\n",
    "    if not search_bar:\n",
    "        print(\"Search bar not found\")\n",
    "        return None\n",
    "\n",
    "    # Type the search query\n",
    "    search_bar['value'] = '50 most expensive cars'\n",
    "\n",
    "    # Submit the search form\n",
    "    form = search_bar.find_parent(\"form\")\n",
    "    response = requests.post(url, data=form.attrs['data-search'])\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    # Parse the HTML content of the search results page\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Click on the link for the 50 most expensive cars\n",
    "    link = soup.find(\"a\", text=\"50 most expensive cars in the world..\")\n",
    "    if not link:\n",
    "        print(\"Link not found\")\n",
    "        return None\n",
    "\n",
    "    link_url = link['href']\n",
    "    response = requests.get(link_url)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    # Parse the HTML content of the page with the 50 most expensive cars\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find the table containing the data\n",
    "    table = soup.find(\"table\", class_=\"table table-stripe table-bordered table-hover\")\n",
    "\n",
    "    # Initialize lists to store data\n",
    "    car_names = []\n",
    "    prices = []\n",
    "\n",
    "    # Extract data from the table rows\n",
    "    rows = table.find_all(\"tr\")[1:]  # Exclude header row\n",
    "    for row in rows:\n",
    "        data = row.find_all(\"td\")\n",
    "        car_name = data[0].text.strip()\n",
    "        price = data[1].text.strip()\n",
    "        car_names.append(car_name)\n",
    "        prices.append(price)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Car Name\": car_names,\n",
    "        \"Price\": prices\n",
    "    })\n",
    "\n",
    "    return df\n",
    "\n",
    "# Scrape data for the 50 most expensive cars in the world\n",
    "df_expensive_cars = scrape_expensive_cars()\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_expensive_cars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0afdd16-c5a9-4abe-be30-6537acf09a92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
