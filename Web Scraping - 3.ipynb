{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f1fbfab-3f0a-480b-8e7d-ea0a9f9ae97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62c19c82-8461-4124-a281-86b8eb140605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for on Amazon:  phone\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "503 Server Error: Service Unavailable for url: https://www.amazon.in/s?k=phone",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     22\u001b[0m     user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the product you want to search for on Amazon: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m     search_products(user_input)\n",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m, in \u001b[0;36msearch_products\u001b[1;34m(product)\u001b[0m\n\u001b[0;32m      6\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m      8\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m----> 9\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for unsuccessful status codes\u001b[39;00m\n\u001b[0;32m     11\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     12\u001b[0m products \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma-size-medium a-color-base a-text-normal\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[1;32mE:\\Data Science\\Anaconda\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://www.amazon.in/s?k=phone"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def search_products(product):\n",
    "    url = f\"https://www.amazon.in/s?k={product}\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    products = soup.find_all('span', {'class': 'a-size-medium a-color-base a-text-normal'})\n",
    "\n",
    "    if products:\n",
    "        print(f\"Products related to '{product}':\\n\")\n",
    "        for idx, item in enumerate(products, 1):\n",
    "            print(f\"{idx}. {item.text.strip()}\")\n",
    "    else:\n",
    "        print(f\"No products found for '{product}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    user_input = input(\"Enter the product you want to search for on Amazon: \")\n",
    "    search_products(user_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9680dc28-b92c-4e8f-af94-5ebe2ba6e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "327d770c-3e3c-4ec2-9a1c-f021dab82813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the product you want to search for on Amazon:  guitar\n",
      "Enter the number of pages to scrape (default is 3):  3\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "503 Server Error: Service Unavailable for url: https://www.amazon.in/s?k=guitar&page=1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 86\u001b[0m\n\u001b[0;32m     83\u001b[0m product \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the product you want to search for on Amazon: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     84\u001b[0m pages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the number of pages to scrape (default is 3): \u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 86\u001b[0m products_data \u001b[38;5;241m=\u001b[39m scrape_products(product, pages)\n\u001b[0;32m     87\u001b[0m save_to_csv(products_data, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproduct\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_products.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 65\u001b[0m, in \u001b[0;36mscrape_products\u001b[1;34m(product, pages)\u001b[0m\n\u001b[0;32m     62\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     64\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m---> 65\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \u001b[38;5;66;03m# Raise an exception for unsuccessful status codes\u001b[39;00m\n\u001b[0;32m     67\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     68\u001b[0m products \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma-size-medium a-color-base a-text-normal\u001b[39m\u001b[38;5;124m'\u001b[39m})\n",
      "File \u001b[1;32mE:\\Data Science\\Anaconda\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 503 Server Error: Service Unavailable for url: https://www.amazon.in/s?k=guitar&page=1"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_product_details(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Extracting details\n",
    "    try:\n",
    "        brand_name = soup.find('a', class_='a-link-normal').text.strip()\n",
    "    except AttributeError:\n",
    "        brand_name = '-'\n",
    "\n",
    "    try:\n",
    "        product_name = soup.find('span', id='productTitle').text.strip()\n",
    "    except AttributeError:\n",
    "        product_name = '-'\n",
    "\n",
    "    try:\n",
    "        price = soup.find('span', id='priceblock_ourprice').text.strip()\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            price = soup.find('span', id='priceblock_dealprice').text.strip()\n",
    "        except AttributeError:\n",
    "            price = '-'\n",
    "\n",
    "    try:\n",
    "        return_exchange = soup.find('div', {'id': 'RETURNS_POLICY'}).text.strip()\n",
    "    except AttributeError:\n",
    "        return_exchange = '-'\n",
    "\n",
    "    try:\n",
    "        expected_delivery = soup.find('span', {'id': 'ddmDeliveryMessage'}).text.strip()\n",
    "    except AttributeError:\n",
    "        expected_delivery = '-'\n",
    "\n",
    "    try:\n",
    "        availability = soup.find('div', {'id': 'availability'}).text.strip()\n",
    "    except AttributeError:\n",
    "        availability = '-'\n",
    "\n",
    "    return {\n",
    "        'Brand Name': brand_name,\n",
    "        'Name of the Product': product_name,\n",
    "        'Price': price,\n",
    "        'Return/Exchange': return_exchange,\n",
    "        'Expected Delivery': expected_delivery,\n",
    "        'Availability': availability,\n",
    "        'Product URL': url\n",
    "    }\n",
    "\n",
    "def scrape_products(product, pages=3):\n",
    "    data = []\n",
    "\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"https://www.amazon.in/s?k={product}&page={page}\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        products = soup.find_all('span', {'class': 'a-size-medium a-color-base a-text-normal'})\n",
    "\n",
    "        for product in products:\n",
    "            product_url = \"https://www.amazon.in\" + product.find_parent('a')['href']\n",
    "            product_data = scrape_product_details(product_url)\n",
    "            data.append(product_data)\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    product = input(\"Enter the product you want to search for on Amazon: \")\n",
    "    pages = int(input(\"Enter the number of pages to scrape (default is 3): \") or \"3\")\n",
    "\n",
    "    products_data = scrape_products(product, pages)\n",
    "    save_to_csv(products_data, f\"{product}_products.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e170c7c-9233-43c9-a20a-0d68c47c8746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84588a4a-5568-4de8-92d0-e41937f64752",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeys\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Keys\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_images(keyword, num_images):\n",
    "    # Create a new instance of Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open Google Images\n",
    "    driver.get(\"https://images.google.com/\")\n",
    "\n",
    "    # Find the search bar\n",
    "    search_bar = driver.find_element_by_name(\"q\")\n",
    "\n",
    "    # Enter the keyword and submit the search\n",
    "    search_bar.send_keys(keyword)\n",
    "    search_bar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Scroll down to load more images\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Get the page source and parse it with BeautifulSoup\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "    # Find all image elements\n",
    "    images = soup.find_all(\"img\", class_=\"rg_i\")\n",
    "\n",
    "    # Create a directory to save the images\n",
    "    os.makedirs(keyword, exist_ok=True)\n",
    "\n",
    "    # Download the images\n",
    "    for idx, image in enumerate(images[:num_images], 1):\n",
    "        image_url = image['src']\n",
    "        image_data = requests.get(image_url).content\n",
    "        with open(os.path.join(keyword, f\"{keyword}_{idx}.jpg\"), \"wb\") as f:\n",
    "            f.write(image_data)\n",
    "            print(f\"Downloaded {keyword} image {idx}\")\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "# List of keywords and number of images to scrape for each keyword\n",
    "keywords = ['fruits', 'cars', 'Machine Learning', 'Guitar', 'Cakes']\n",
    "num_images_per_keyword = 10\n",
    "\n",
    "# Scrape images for each keyword\n",
    "for keyword in keywords:\n",
    "    scrape_images(keyword, num_images_per_keyword)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "10daea64-20c3-4ce5-8676-ebcefe6f08f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b82c6425-3814-4202-80af-d34794fdba7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the smartphone you want to search for on Flipkart:  apple\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     88\u001b[0m     search_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter the smartphone you want to search for on Flipkart: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 89\u001b[0m     smartphone_data \u001b[38;5;241m=\u001b[39m scrape_smartphone_details(search_query)\n\u001b[0;32m     90\u001b[0m     save_to_csv(smartphone_data, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msearch_query\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_smartphones.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m products:\n",
      "Cell \u001b[1;32mIn[12], line 62\u001b[0m, in \u001b[0;36mscrape_smartphone_details\u001b[1;34m(search_query)\u001b[0m\n\u001b[0;32m     59\u001b[0m     price \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 62\u001b[0m     product_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.flipkart.com\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m product\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m, {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIRpwTa\u001b[39m\u001b[38;5;124m'\u001b[39m})[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_smartphone_details(search_query):\n",
    "    url = f\"https://www.flipkart.com/search?q={search_query}&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    products = soup.find_all('div', {'class': '_1AtVbE'})\n",
    "\n",
    "    data = []\n",
    "    for product in products:\n",
    "        try:\n",
    "            brand_name = product.find('div', {'class': '_4rR01T'}).text\n",
    "        except AttributeError:\n",
    "            brand_name = '-'\n",
    "\n",
    "        try:\n",
    "            smartphone_name = product.find('a', {'class': 'IRpwTa'}).text\n",
    "        except AttributeError:\n",
    "            smartphone_name = '-'\n",
    "\n",
    "        try:\n",
    "            color = product.find('div', {'class': '_3LWrw9'}).text\n",
    "        except AttributeError:\n",
    "            color = '-'\n",
    "\n",
    "        specs = product.find_all('li', {'class': 'rgWa7D'})\n",
    "        ram = '-'\n",
    "        rom = '-'\n",
    "        primary_camera = '-'\n",
    "        secondary_camera = '-'\n",
    "        display_size = '-'\n",
    "        battery_capacity = '-'\n",
    "        price = '-'\n",
    "        product_url = '-'\n",
    "        for spec in specs:\n",
    "            if 'RAM' in spec.text:\n",
    "                ram = spec.text.split('|')[0].strip()\n",
    "            elif 'ROM' in spec.text:\n",
    "                rom = spec.text.split('|')[0].strip()\n",
    "            elif 'MP' in spec.text:\n",
    "                if 'Primary' in spec.text:\n",
    "                    primary_camera = spec.text.split('|')[0].strip()\n",
    "                elif 'Secondary' in spec.text:\n",
    "                    secondary_camera = spec.text.split('|')[0].strip()\n",
    "            elif 'Display Size' in spec.text:\n",
    "                display_size = spec.text.split('|')[0].strip()\n",
    "            elif 'Battery' in spec.text:\n",
    "                battery_capacity = spec.text.split('|')[0].strip()\n",
    "\n",
    "        try:\n",
    "            price = product.find('div', {'class': '_30jeq3 _1_WHN1'}).text\n",
    "        except AttributeError:\n",
    "            price = '-'\n",
    "\n",
    "        try:\n",
    "            product_url = \"https://www.flipkart.com\" + product.find('a', {'class': 'IRpwTa'})['href']\n",
    "        except AttributeError:\n",
    "            pass\n",
    "\n",
    "        data.append({\n",
    "            'Brand Name': brand_name,\n",
    "            'Smartphone Name': smartphone_name,\n",
    "            'Colour': color,\n",
    "            'RAM': ram,\n",
    "            'Storage (ROM)': rom,\n",
    "            'Primary Camera': primary_camera,\n",
    "            'Secondary Camera': secondary_camera,\n",
    "            'Display Size': display_size,\n",
    "            'Battery Capacity': battery_capacity,\n",
    "            'Price': price,\n",
    "            'Product URL': product_url\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Data saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    search_query = input(\"Enter the smartphone you want to search for on Flipkart: \")\n",
    "    smartphone_data = scrape_smartphone_details(search_query)\n",
    "    save_to_csv(smartphone_data, f\"{search_query}_smartphones.csv\")\n",
    "for product in products:\n",
    "    try:\n",
    "        brand_name = product.find('div', {'class': '_4rR01T'}).text\n",
    "    except AttributeError:\n",
    "        brand_name = '-'\n",
    "\n",
    "    try:\n",
    "        smartphone_name = product.find('a', {'class': 'IRpwTa'}).text\n",
    "    except AttributeError:\n",
    "        smartphone_name = '-'\n",
    "\n",
    "    try:\n",
    "        color = product.find('div', {'class': '_3LWrw9'}).text\n",
    "    except AttributeError:\n",
    "        color = '-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f95210f-aa7c-4610-ac5d-20a5c0ffe5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e4c9da7-e6fa-4179-b15f-0ea1980ea355",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'selenium'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mselenium\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m webdriver\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_coordinates\u001b[39m(city):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Create a new instance of Chrome WebDriver\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'selenium'"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "def get_coordinates(city):\n",
    "    # Create a new instance of Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Open Google Maps\n",
    "    driver.get(\"https://www.google.com/maps\")\n",
    "\n",
    "    # Find the search bar and enter the city\n",
    "    search_bar = driver.find_element_by_id(\"searchboxinput\")\n",
    "    search_bar.send_keys(city)\n",
    "\n",
    "    # Submit the search\n",
    "    search_bar.submit()\n",
    "\n",
    "    # Wait for the page to load\n",
    "    driver.implicitly_wait(10)  # Adjust the wait time as needed\n",
    "\n",
    "    # Get the current URL which contains the coordinates\n",
    "    url = driver.current_url\n",
    "\n",
    "    # Parse the coordinates from the URL\n",
    "    coordinates = parse_coordinates_from_url(url)\n",
    "\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n",
    "\n",
    "    return coordinates\n",
    "\n",
    "def parse_coordinates_from_url(url):\n",
    "    # Extract latitude and longitude from the URL\n",
    "    # Example URL: https://www.google.com/maps/place/latitude,longitude\n",
    "    # Example URL: https://www.google.com/maps/place/37.7749,-122.4194\n",
    "    parts = url.split(\"/place/\")[-1].split(\",\")\n",
    "    latitude = parts[0]\n",
    "    longitude = parts[1]\n",
    "\n",
    "    return latitude, longitude\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    city = input(\"Enter the city to search for on Google Maps: \")\n",
    "    coordinates = get_coordinates(city)\n",
    "    print(f\"Coordinates for {city}: Latitude - {coordinates[0]}, Longitude - {coordinates[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4282a2ce-e038-4396-b660-5085c89ace4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fb5cf6e-549f-4ced-a774-502730b86999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_gaming_laptops():\n",
    "    url = \"https://www.digit.in/top-products/best-gaming-laptops-40.html\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    laptops = []\n",
    "    for laptop in soup.find_all('div', class_='TopNumbeHeading sticky-footer'):\n",
    "        laptop_details = {}\n",
    "        try:\n",
    "            laptop_details['Name'] = laptop.find('h3').text.strip()\n",
    "        except AttributeError:\n",
    "            laptop_details['Name'] = '-'\n",
    "\n",
    "        specs = laptop.find_next_sibling('div', class_='product-detail')\n",
    "        if specs:\n",
    "            try:\n",
    "                laptop_details['Specifications'] = specs.find('div', class_='value').text.strip()\n",
    "            except AttributeError:\n",
    "                laptop_details['Specifications'] = '-'\n",
    "        else:\n",
    "            laptop_details['Specifications'] = '-'\n",
    "\n",
    "        rating = laptop.find_next_sibling('div', class_='rating')\n",
    "        if rating:\n",
    "            try:\n",
    "                laptop_details['Rating'] = rating.find('p').text.strip()\n",
    "            except AttributeError:\n",
    "                laptop_details['Rating'] = '-'\n",
    "        else:\n",
    "            laptop_details['Rating'] = '-'\n",
    "\n",
    "        laptops.append(laptop_details)\n",
    "\n",
    "    return laptops\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gaming_laptops = scrape_gaming_laptops()\n",
    "    for laptop in gaming_laptops:\n",
    "        print(laptop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97585091-2367-488f-807f-ac1c9d06c764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "893ed2e6-4e5d-48fe-bb54-ca081a2e7b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_billionaires():\n",
    "    url = \"https://www.forbes.com/billionaires/\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    billionaires = []\n",
    "    for person in soup.find_all('div', class_='personList'):\n",
    "        billionaire_details = {}\n",
    "        try:\n",
    "            billionaire_details['Rank'] = person.find('div', class_='rank').text.strip()\n",
    "        except AttributeError:\n",
    "            billionaire_details['Rank'] = '-'\n",
    "\n",
    "        try:\n",
    "            billionaire_details['Name'] = person.find('div', class_='personName').text.strip()\n",
    "        except AttributeError:\n",
    "            billionaire_details['Name'] = '-'\n",
    "\n",
    "        try:\n",
    "            billionaire_details['Net Worth'] = person.find('div', class_='netWorth').text.strip()\n",
    "        except AttributeError:\n",
    "            billionaire_details['Net Worth'] = '-'\n",
    "\n",
    "        try:\n",
    "            billionaire_details['Age'] = person.find('div', class_='age').text.strip()\n",
    "        except AttributeError:\n",
    "            billionaire_details['Age'] = '-'\n",
    "\n",
    "        try:\n",
    "            billionaire_details['Citizenship'] = person.find('div', class_='countryOfCitizenship').text.strip()\n",
    "        except AttributeError:\n",
    "            billionaire_details['Citizenship'] = '-'\n",
    "\n",
    "        try:\n",
    "            billionaire_details['Source'] = person.find('div', class_='source').text.strip()\n",
    "        except AttributeError:\n",
    "            billionaire_details['Source'] = '-'\n",
    "\n",
    "        try:\n",
    "            billionaire_details['Industry'] = person.find('div', class_='category').text.strip()\n",
    "        except AttributeError:\n",
    "            billionaire_details['Industry'] = '-'\n",
    "\n",
    "        billionaires.append(billionaire_details)\n",
    "\n",
    "    return billionaires\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    billionaires = scrape_billionaires()\n",
    "    for person in billionaires:\n",
    "        print(person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48440099-c9ea-41bd-ba46-99fd78f425ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d41b3267-fcd6-434e-917f-9d72364219f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'googleapiclient'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgoogleapiclient\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Set up YouTube Data API credentials\u001b[39;00m\n\u001b[0;32m      5\u001b[0m API_KEY \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYOUTUBE_API_KEY\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Replace with your API key\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'googleapiclient'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import googleapiclient.discovery\n",
    "\n",
    "# Set up YouTube Data API credentials\n",
    "API_KEY = os.environ.get('YOUTUBE_API_KEY')  # Replace with your API key\n",
    "\n",
    "def fetch_comments(video_id, max_results=100):\n",
    "    youtube = googleapiclient.discovery.build('youtube', 'v3', developerKey=API_KEY)\n",
    "    comments = []\n",
    "\n",
    "    nextPageToken = None\n",
    "    while True:\n",
    "        response = youtube.commentThreads().list(\n",
    "            part=\"snippet\",\n",
    "            videoId=video_id,\n",
    "            maxResults=min(max_results, 100),\n",
    "            pageToken=nextPageToken\n",
    "        ).execute()\n",
    "\n",
    "        for item in response['items']:\n",
    "            comment = item['snippet']['topLevelComment']['snippet']\n",
    "            comment_text = comment['textDisplay']\n",
    "            like_count = comment['likeCount']\n",
    "            published_at = comment['publishedAt']\n",
    "            comments.append({'text': comment_text, 'likes': like_count, 'published_at': published_at})\n",
    "\n",
    "        nextPageToken = response.get('nextPageToken')\n",
    "        if not nextPageToken or len(comments) >= max_results:\n",
    "            break\n",
    "\n",
    "    return comments\n",
    "\n",
    "def main():\n",
    "    video_id = input(\"Enter the video ID of the YouTube video: \")\n",
    "    comments = fetch_comments(video_id, max_results=500)\n",
    "\n",
    "    print(f\"Total comments extracted: {len(comments)}\")\n",
    "    for comment in comments:\n",
    "        print(f\"Comment: {comment['text']}\")\n",
    "        print(f\"Likes: {comment['likes']}\")\n",
    "        print(f\"Published at: {comment['published_at']}\")\n",
    "        print('-' * 50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf1e0eea-988d-41ca-9d00-cfcf7fe1d611",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "09c3b5b0-68ac-4d5b-9139-91ad3e46cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrape_hostels(location):\n",
    "    url = f\"https://www.hostelworld.com/search?search_keywords={location}&country=England&city=London&date_from=2022-04-15&date_to=2022-04-18&number_of_guests=1\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  # Raise an exception for unsuccessful status codes\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    hostels = []\n",
    "    for hostel in soup.find_all('div', class_='fabresult'):\n",
    "        hostel_details = {}\n",
    "        hostel_details['Name'] = hostel.find('h2', class_='title').text.strip()\n",
    "        hostel_details['Distance from City Centre'] = hostel.find('span', class_='description').text.strip()\n",
    "        hostel_details['Ratings'] = hostel.find('div', class_='score').text.strip()\n",
    "        hostel_details['Total Reviews'] = hostel.find('div', class_='reviews').text.strip()\n",
    "        hostel_details['Overall Reviews'] = hostel.find('div', class_='keyword').text.strip()\n",
    "        hostel_details['Privates From Price'] = hostel.find('div', class_='prices').find('span', class_='price').text.strip()\n",
    "        hostel_details['Dorms From Price'] = hostel.find('div', class_='prices').find_all('span', class_='price')[1].text.strip()\n",
    "        hostel_details['Facilities'] = [facility.text.strip() for facility in hostel.find('ul', class_='facilities').find_all('li')]\n",
    "        hostel_details['Property Description'] = hostel.find('div', class_='ratingdescription').text.strip()\n",
    "        hostels.append(hostel_details)\n",
    "\n",
    "    return hostels\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    location = \"London\"\n",
    "    hostels = scrape_hostels(location)\n",
    "    for hostel in hostels:\n",
    "        print(hostel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07397fa1-2065-4545-8cc2-8cb043d79768",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
