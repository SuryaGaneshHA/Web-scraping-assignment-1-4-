{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4704f4e-9e00-4442-9ae5-0480966a133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1 Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c68e2c2-8f7b-4020-9cc1-5183a3c6eb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Headers\n",
      "0                       Main Page\n",
      "1            Welcome to Wikipedia\n",
      "2   From today's featured article\n",
      "3                Did you knowÂ ...\n",
      "4                     In the news\n",
      "5                     On this day\n",
      "6      From today's featured list\n",
      "7        Today's featured picture\n",
      "8        Other areas of Wikipedia\n",
      "9     Wikipedia's sister projects\n",
      "10            Wikipedia languages\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_wikipedia_headers(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    headers = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])\n",
    "    \n",
    "    header_texts = [header.text.strip() for header in headers]\n",
    "    \n",
    "    return header_texts\n",
    "\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
    "\n",
    "headers = get_wikipedia_headers(wiki_url)\n",
    "\n",
    "df = pd.DataFrame({'Headers': headers})\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ddd11f4-a841-49bc-9939-212ac0e2b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2  Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffab3661-c4bb-46f7-895a-7a2b28ec2369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 404 Client Error: Not Found for url: https://presidentofindia.nic.in/former-presidents.htm\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def get_former_presidents(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table', class_='table')\n",
    "        if not table:\n",
    "            print(\"Table not found on the page.\")\n",
    "            return []\n",
    "        \n",
    "        presidents_data = []\n",
    "        for row in table.find_all('tr')[1:]:\n",
    "            columns = row.find_all('td')\n",
    "            name = columns[0].text.strip()\n",
    "            term_of_office = columns[1].text.strip()\n",
    "            presidents_data.append({'Name': name, 'Term of Office': term_of_office})\n",
    "        \n",
    "        return presidents_data\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "        return []\n",
    "\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "\n",
    "former_presidents = get_former_presidents(url)\n",
    "\n",
    "df = pd.DataFrame(former_presidents)\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89b68640-9b5a-4d23-9aad-b70489dcb45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4 Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1eb9cdc8-b8ca-4b9d-9168-2af78dafa7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams in Women's Cricket:\n",
      "Empty DataFrame\n",
      "Columns: [Team, Matches, Points, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 Women's ODI Batting Players:\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 Women's ODI All-Rounders:\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_womens_odi_teams():\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    \n",
    "    for team in soup.find_all('tr', class_='table-body'):\n",
    "        teams.append(team.find('span', class_='u-hide-phablet').text.strip())\n",
    "        matches.append(team.find_all('td')[2].text.strip())\n",
    "        points.append(team.find_all('td')[3].text.strip())\n",
    "        ratings.append(team.find_all('td')[4].text.strip())\n",
    "    \n",
    "    df_teams = pd.DataFrame({\n",
    "        'Team': teams[:10],\n",
    "        'Matches': matches[:10],\n",
    "        'Points': points[:10],\n",
    "        'Rating': ratings[:10]\n",
    "    })\n",
    "    \n",
    "    return df_teams\n",
    "\n",
    "def scrape_womens_odi_batting():\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    for player in soup.find_all('tr', class_='table-body'):\n",
    "        players.append(player.find('td', class_='table-body__cell name').text.strip())\n",
    "        teams.append(player.find('span', class_='table-body__logo-text').text.strip())\n",
    "        ratings.append(player.find('td', class_='table-body__cell u-text-right rating').text.strip())\n",
    "    \n",
    "    df_batting = pd.DataFrame({\n",
    "        'Player': players[:10],\n",
    "        'Team': teams[:10],\n",
    "        'Rating': ratings[:10]\n",
    "    })\n",
    "    \n",
    "    return df_batting\n",
    "\n",
    "def scrape_womens_odi_allrounders():\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    for player in soup.find_all('tr', class_='table-body'):\n",
    "        players.append(player.find('td', class_='table-body__cell name').text.strip())\n",
    "        teams.append(player.find('span', class_='table-body__logo-text').text.strip())\n",
    "        ratings.append(player.find('td', class_='table-body__cell u-text-right rating').text.strip())\n",
    "    \n",
    "    df_allrounders = pd.DataFrame({\n",
    "        'Player': players[:10],\n",
    "        'Team': teams[:10],\n",
    "        'Rating': ratings[:10]\n",
    "    })\n",
    "    \n",
    "    return df_allrounders\n",
    "\n",
    "def main():\n",
    "    df_teams = scrape_womens_odi_teams()\n",
    "    df_batting = scrape_womens_odi_batting()\n",
    "    df_allrounders = scrape_womens_odi_allrounders()\n",
    "    \n",
    "    print(\"Top 10 ODI Teams in Women's Cricket:\")\n",
    "    print(df_teams)\n",
    "    print(\"\\nTop 10 Women's ODI Batting Players:\")\n",
    "    print(df_batting)\n",
    "    print(\"\\nTop 10 Women's ODI All-Rounders:\")\n",
    "    print(df_allrounders)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3d14fac7-d3ce-4724-8db0-08d8e767ffe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3  Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5634226f-0d94-4d9b-88ee-e0f187435951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams in Men's Cricket:\n",
      "Empty DataFrame\n",
      "Columns: [Team, Matches, Points, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "Empty DataFrame\n",
      "Columns: [Player, Team, Rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_mens_odi_teams():\n",
    "    url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    \n",
    "    for team in soup.find_all('tr', class_='table-body'):\n",
    "        teams.append(team.find('span', class_='u-hide-phablet').text.strip())\n",
    "        matches.append(team.find_all('td')[2].text.strip())\n",
    "        points.append(team.find_all('td')[3].text.strip())\n",
    "        ratings.append(team.find_all('td')[4].text.strip())\n",
    "    \n",
    "    df_teams = pd.DataFrame({\n",
    "        'Team': teams[:10],\n",
    "        'Matches': matches[:10],\n",
    "        'Points': points[:10],\n",
    "        'Rating': ratings[:10]\n",
    "    })\n",
    "    \n",
    "    return df_teams\n",
    "\n",
    "def scrape_mens_odi_batsmen():\n",
    "    url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    for player in soup.find_all('tr', class_='table-body'):\n",
    "        players.append(player.find('td', class_='table-body__cell name').text.strip())\n",
    "        teams.append(player.find('span', class_='table-body__logo-text').text.strip())\n",
    "        ratings.append(player.find('td', class_='table-body__cell u-text-right rating').text.strip())\n",
    "    \n",
    "    df_batsmen = pd.DataFrame({\n",
    "        'Player': players[:10],\n",
    "        'Team': teams[:10],\n",
    "        'Rating': ratings[:10]\n",
    "    })\n",
    "    \n",
    "    return df_batsmen\n",
    "\n",
    "def scrape_mens_odi_bowlers():\n",
    "    url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    for player in soup.find_all('tr', class_='table-body'):\n",
    "        players.append(player.find('td', class_='table-body__cell name').text.strip())\n",
    "        teams.append(player.find('span', class_='table-body__logo-text').text.strip())\n",
    "        ratings.append(player.find('td', class_='table-body__cell u-text-right rating').text.strip())\n",
    "    \n",
    "    df_bowlers = pd.DataFrame({\n",
    "        'Player': players[:10],\n",
    "        'Team': teams[:10],\n",
    "        'Rating': ratings[:10]\n",
    "    })\n",
    "    \n",
    "    return df_bowlers\n",
    "\n",
    "def main():\n",
    "    df_teams = scrape_mens_odi_teams()\n",
    "    df_batsmen = scrape_mens_odi_batsmen()\n",
    "    df_bowlers = scrape_mens_odi_bowlers()\n",
    "    \n",
    "    print(\"Top 10 ODI Teams in Men's Cricket:\")\n",
    "    print(df_teams)\n",
    "    print(\"\\nTop 10 ODI Batsmen:\")\n",
    "    print(df_batsmen)\n",
    "    print(\"\\nTop 10 ODI Bowlers:\")\n",
    "    print(df_bowlers)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07a6a17-0269-460d-bcea-ff6b756c0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dd0ed54a-ace3-4a7c-905b-b1e87e5a7693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNBC World News:\n",
      "   Headline Time News Link\n",
      "0       N/A  N/A       N/A\n",
      "1       N/A  N/A       N/A\n",
      "2       N/A  N/A       N/A\n",
      "3       N/A  N/A       N/A\n",
      "4       N/A  N/A       N/A\n",
      "5       N/A  N/A       N/A\n",
      "6       N/A  N/A       N/A\n",
      "7       N/A  N/A       N/A\n",
      "8       N/A  N/A       N/A\n",
      "9       N/A  N/A       N/A\n",
      "10      N/A  N/A       N/A\n",
      "11      N/A  N/A       N/A\n",
      "12      N/A  N/A       N/A\n",
      "13      N/A  N/A       N/A\n",
      "14      N/A  N/A       N/A\n",
      "15      N/A  N/A       N/A\n",
      "16      N/A  N/A       N/A\n",
      "17      N/A  N/A       N/A\n",
      "18      N/A  N/A       N/A\n",
      "19      N/A  N/A       N/A\n",
      "20      N/A  N/A       N/A\n",
      "21      N/A  N/A       N/A\n",
      "22      N/A  N/A       N/A\n",
      "23      N/A  N/A       N/A\n",
      "24      N/A  N/A       N/A\n",
      "25      N/A  N/A       N/A\n",
      "26      N/A  N/A       N/A\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_cnbc_world_news():\n",
    "    url = 'https://www.cnbc.com/world/?region=world'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    headlines = []\n",
    "    times = []\n",
    "    news_links = []\n",
    "    \n",
    "    articles = soup.find_all('div', class_='Card-titleContainer')\n",
    "    \n",
    "    for article in articles:\n",
    "        try:\n",
    "            headline = article.find('a', class_='Card-titleLink').text.strip()\n",
    "        except AttributeError:\n",
    "            headline = \"N/A\"\n",
    "        headlines.append(headline)\n",
    "        \n",
    "        try:\n",
    "            time = article.find('time').text.strip()\n",
    "        except AttributeError:\n",
    "            time = \"N/A\"\n",
    "        times.append(time)\n",
    "        \n",
    "        try:\n",
    "            news_link = 'https://www.cnbc.com' + article.find('a', class_='Card-titleLink')['href']\n",
    "        except (AttributeError, TypeError):\n",
    "            news_link = \"N/A\"\n",
    "        news_links.append(news_link)\n",
    "    \n",
    "    df_news = pd.DataFrame({\n",
    "        'Headline': headlines,\n",
    "        'Time': times,\n",
    "        'News Link': news_links\n",
    "    })\n",
    "    \n",
    "    return df_news\n",
    "\n",
    "def main():\n",
    "    df_news = scrape_cnbc_world_news()\n",
    "    print(\"CNBC World News:\")\n",
    "    print(df_news)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "960213ca-5e8b-4745-9e06-ce041a6bc72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c3e86df5-5216-4fc7-8533-885e781d4810",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://www.sciencedirect.com/unsupported_browser",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 47\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     45\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 47\u001b[0m df_articles \u001b[38;5;241m=\u001b[39m scrape_most_downloaded_articles(url)\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_articles)\n",
      "Cell \u001b[1;32mIn[30], line 11\u001b[0m, in \u001b[0;36mscrape_most_downloaded_articles\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m      6\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUser-Agent\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      8\u001b[0m }\n\u001b[0;32m     10\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(url, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m---> 11\u001b[0m response\u001b[38;5;241m.\u001b[39mraise_for_status()  \n\u001b[0;32m     13\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     15\u001b[0m articles \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124marticle-list-item\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Data Science\\Anaconda\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1016\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreason\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1018\u001b[0m     )\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 400 Client Error: Bad Request for url: https://www.sciencedirect.com/unsupported_browser"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_most_downloaded_articles(url):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()  \n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    articles = soup.find_all('li', class_='article-list-item')\n",
    "    \n",
    "    titles = []\n",
    "    authors = []\n",
    "    published_dates = []\n",
    "    paper_urls = []\n",
    "    \n",
    "    for article in articles:\n",
    "        title = article.find('a', class_='article-item-title').text.strip()\n",
    "        titles.append(title)\n",
    "        \n",
    "        author_list = article.find('ul', class_='author-list')\n",
    "        author_names = [author.text.strip() for author in author_list.find_all('li')]\n",
    "        authors.append(', '.join(author_names))\n",
    "        \n",
    "        published_date = article.find('div', class_='js-article-date').text.strip()\n",
    "        published_dates.append(published_date)\n",
    "        \n",
    "        paper_url = 'https://www.journals.elsevier.com' + article.find('a', class_='article-item-title')['href']\n",
    "        paper_urls.append(paper_url)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Paper Title': titles,\n",
    "        'Authors': authors,\n",
    "        'Published Date': published_dates,\n",
    "        'Paper URL': paper_urls\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "url = 'https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles'\n",
    "\n",
    "df_articles = scrape_most_downloaded_articles(url)\n",
    "\n",
    "print(df_articles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0a8744d-4f5b-4df3-84e9-668505188af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8101280-f93b-4bb6-80b7-7e51f809eb80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Restaurant Name, Cuisine, Location, Ratings, Image URL]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_restaurant_details(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  \n",
    "    \n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    restaurant_cards = soup.find_all('div', class_='restnt-card-container')\n",
    "    \n",
    "    restaurant_names = []\n",
    "    cuisines = []\n",
    "    locations = []\n",
    "    ratings = []\n",
    "    image_urls = []\n",
    "    \n",
    "    for card in restaurant_cards:\n",
    "        name = card.find('div', class_='restnt-card-wrap').find('div', class_='restnt-card-title').text.strip()\n",
    "        restaurant_names.append(name)\n",
    "        \n",
    "        cuisine = card.find('div', class_='restnt-card-wrap').find('div', class_='restnt-card-cuisines').text.strip()\n",
    "        cuisines.append(cuisine)\n",
    "        \n",
    "        location = card.find('div', class_='restnt-card-wrap').find('div', class_='restnt-card-info').text.strip()\n",
    "        locations.append(location)\n",
    "        \n",
    "        rating = card.find('div', class_='restnt-card-wrap').find('span', class_='restnt-rating').text.strip()\n",
    "        ratings.append(rating)\n",
    "        \n",
    "        image_url = card.find('div', class_='restnt-card-image').find('img')['data-original']\n",
    "        image_urls.append(image_url)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Restaurant Name': restaurant_names,\n",
    "        'Cuisine': cuisines,\n",
    "        'Location': locations,\n",
    "        'Ratings': ratings,\n",
    "        'Image URL': image_urls\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "url = 'https://www.dineout.co.in/delhi-restaurants/buffet-special'\n",
    "\n",
    "df_restaurants = scrape_restaurant_details(url)\n",
    "\n",
    "print(df_restaurants)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f9326b-8772-4b9a-9747-acd55c603f22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
